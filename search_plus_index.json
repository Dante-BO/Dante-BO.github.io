{"./":{"url":"./","title":"写在前面","keywords":"","body":"写在前面 大家好啊，我是DanteSU，一个梦想是成为计算机科学家的初学者 本网站主要是用来记录我的一些学习经历和试错经验，当然还有一些有用的链接。 emmm 大概就是这样吧，有问题欢迎随时通过 Github 与我联系。 Test部分 解释：在写笔记的时候遇到的很多bug会在这里做测试，请勿相信以下字符。。。 one linethe other line one linethe other line DanteSU            updated 2022-06-05 20:51:40 "},"Links/":{"url":"Links/","title":"常用链接","keywords":"","body":"常用链接 Main Stream 知乎 https://www.zhihu.com/ Bilibili https://www.bilibili.com/ ML Baidu AI Paddle https://aistudio.baidu.com/aistudio/index 动手学深度学习 https://zh-v2.d2l.ai/index.html paperwithcode https://paperswithcode.com/ People 爱可可爱生活的知乎 https://www.zhihu.com/people/fly51fly 台大李宏毅 https://speech.ee.ntu.edu.tw/~hylee/index.php https://www.youtube.com/watch?v=rTqmWlnwz_0 苏剑林的博客 https://kexue.fm/ 豆约翰 https://www.jianshu.com/u/8b23f6864f5d EIC17 KexinTANG https://github.com/Kexin-Tang Entertainment 低端影视 https://ddrk.me/ 美剧7 https://www.meiju7.cc/ Tools 远景论坛 https://bbs.pcbeta.com/ paperyy查重 https://www.paperyy.cn/ 超星大雅查重 http://user.dayainfo.com/show/login v2ray https://github.com/freefq/tutorials https://github.com/wrfree/free JupyterLab，极其强大的下一代notebook！ https://zhuanlan.zhihu.com/p/87403131 English Duolinguo English Test https://englishtest.duolingo.cn/home 登登多邻国 https://det.91ddedu.com/#/ Speak&Improve https://speakandimprove.com/ Write&Improve https://writeandimprove.com/ 招聘 IBM https://www.ibm.com/cn-zh/employment/internship/ 一些链接 名称 网址 新加坡工作指南 https://www.965work.in/archives/work-guide-for-singapore/ DanteSU            updated 2022-06-05 20:40:49 "},"OpCode/":{"url":"OpCode/","title":"操作码","keywords":"","body":"快速操作码 Terminal Anaconda Python Lib Gitbook Markdown Others 你好啊，这里大概是记录一些我平时常用的一些快速操作码，包括服务器终端、conda和一些python的库的等等。。 DanteSU            updated 2022-06-05 21:12:14 "},"OpCode/1.html":{"url":"OpCode/1.html","title":"Terminal","keywords":"","body":"Terminal Author = DanteSU 清屏幕 Windows $ cls Linux/Mac $ clear 查看 查看CUDA版本 $ nvcc -V $ nvcc --version 查看Nvidia显卡状态 $ nvidia-smi 查看显卡设备和显卡的驱动 $ ubuntu-drivers devices 查看是否安装显卡驱动 $ glxinfo | grep rendering 查看cpu信息 $ cat /proc/cpuinfo 查看所有信息 $ cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 查看cpu型号 $ cat /proc/cpuinfo | grep physical | uniq -c 查看物理核心个数 $ getconf LONG_BIT 查看服务器运行在64bit还是32bit 查看内存信息 $ cat /proc/meminfo 查看详细信息 $ free -h 查看大概信息 $ top 动态查看内存使用变化 查看操作系统内核信息 $ uname -a 查看网卡信息 $ dmesg | grep -i eth 离线后台程序 screen 命令 $ apt install screen $ screen -S w1 新建一个w1工作窗口 $ screen -ls 查看当前所有的运行窗口 $ screen -d w1 将w1窗口离线 $ screen -r w1 接入窗口w1 $ ctrl+A+D 退出当前窗口，回到主界面 $ screen -X -S w1 quit 删除w1这个窗口 $ screen kill +编号/名称 删除窗口 tumx 命令 https://zhuanlan.zhihu.com/p/98384704 下载 wget $ wget [option] [url] 基本操作构成 $ wget url 根据文件地址下载 $ wget -O new_name url 使用其它名称保存文件 $ wget -P new_path url 更改文件保存位置 $ wget -c url 中断后的继续下载 $ wget -t 40 url 增加尝试次数（默认20次） $ wget --ftp-user= --ftp-password= url 从FTP加密服务器下载内容 $ wget -U 'xxx' url 模拟浏览器下载 比如 xxx 是 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.43 Safari/537.36 $ wget -b url 后台下载 $ tail -f wget-log 查看后台下载日志 $ vim downloads.txt 下载多个文件 $ wget -i downloads.txt curl For mac or windows $ curl -o name url 根据文件地址下载 $ curl -C - -o name url 断点续传 $ curl --retry 3 -o name url 调整尝试次数 zip $ unzip [-cflptuvz][-agCjLMnoqsVX][-P ][.zip文件][文件][-d ][-x ] 或 unzip [-Z] -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。 -f 更新现有的文件。 -l 显示压缩文件内所包含的文件。 -p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。 -t 检查压缩文件是否正确。 -u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。 -v 执行时显示详细的信息。 -z 仅显示压缩文件的备注文字。 -a 对文本文件进行必要的字符转换。 -b 不要对文本文件进行字符转换。 -C 压缩文件中的文件名称区分大小写。 -j 不处理压缩文件中原有的目录路径。 -L 将压缩文件中的全部文件名改为小写。 -M 将输出结果送到more程序处理。 -n 解压缩时不要覆盖原有的文件。 -o 不必先询问用户，unzip执行后覆盖原有文件。 -P 使用zip的密码选项。 -q 执行时不显示任何信息。 -s 将文件名中的空白字符转换为底线字符。 -V 保留VMS的文件版本信息。 -X 解压缩时同时回存文件原来的UID/GID。 [.zip文件] 指定.zip压缩文件。 [文件] 指定要处理.zip压缩文件中的哪些文件。 -d 指定文件解压缩后所要存储的目录。 -x 指定不要处理.zip压缩文件中的哪些文件。 -Z unzip -Z等于执行zipinfo指令。 DanteSU            updated 2022-06-05 20:56:23 "},"OpCode/2.html":{"url":"OpCode/2.html","title":"Anaconda","keywords":"","body":"Anaconda Author = DanteSU Conda使用 查看安装的软件包 conda list 查看虚拟环境 conda env list conda info -e 检查更新conda版本 conda update conda 安装与卸载包 conda install package_name=version_in_need conda uninstall package_name 创建与删除虚拟环境 conda create -n env_name (package_name=version_in_need) python=3.7 conda remove -n env_name --all 克隆环境 conda create -n new_env --clone old_env 清理虚拟环境的垃圾 conda clean --all 删除包 conda remove --name env_name package_name 激活与退出虚拟环境 Linux: source activate env_name source deactivate env_name Windows: conda activate conda deactivate env_name pip使用 查看安装包信息（路径、依赖） pip show package_name 一键安装需要的包 requirements.txt 生成 pip freeze >requirements.txt 安装 pip install -r requirements.txt Conda版本 conda install --yes --file requirements.txt environment.yaml activate env_name conda env export > environment.yaml conda env create -f environment.yaml DanteSU            updated 2022-06-05 21:35:15 "},"OpCode/3.html":{"url":"OpCode/3.html","title":"Python Lib","keywords":"","body":"Python Lib Author = DanteSU PyTorch 安装示例 For CUDA 11.3 官方源 conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch -c conda-forge 清华源 地址 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/ https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/linux-64/ 安装 conda install cudatoolkit=11.3 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/ conda install pytorch torchvision torchaudio -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/linux-64/ 注意事项 安装pytorch的时候，在检查安装内容那一项，不要着急输入 yes，先检查一下是否是 GPU 版本的pytorch再安装，如果不是，可以等一会再做尝试或者重新进入需要配置的环0:。1“2境 检验安装成功 $ python import torch torch.cuda.is_available() torch.cuda.get_device_name() torch.__version__ torch.cuda.device_count() SkLearn conda环境 $ conda install scikit-learn pip环境 $ pip3 install scikit-learn cv2 $ pip install opencv-python icrawler https://pypi.org/project/icrawler/0.2.2/ DanteSU            updated 2022-06-05 09:52:07 "},"OpCode/4.html":{"url":"OpCode/4.html","title":"Gitbook","keywords":"","body":"Gitbook Author = DanteSU 安装 Nodejs 因为Gitbook依赖Nodejs，所以首先要安装Nodejs，而且因为gitbook后来很久未更新，所以对新版的nodejs兼容性不好，容易出bug，建议最多使用到第10版的nodejs Mac 从 https://nodejs.org/en/ 下载并安装 Nodejs ，安装完后可通过终端命令 node -v 检验是否安装成功。 后面可能报错，所以可以直接通过 brew 命令下载低版本的 nodejs： $ brew install node@10 $ echo 'export PATH=\"/usr/local/opt/node@10/bin:$PATH\"' >> ~/.zshrc $ source ~/.zshrc $ # 查看版本及是否安装成功 $ node -v $ npm -v Linux Under building... Windows Under building... 安装Gitbook $ npm install gitbook-cli -g # 查看版本号 $ gitbook -V 基本使用 创建新的book $ gitbook init 生成 $ gitbook build 若只执行gitbook build，会生成_book目录，但不能预览。 在这个目录中，对于每一个 markdown 文件都生成了一个相应的 html 文件，同时在 _book/gitbook 文件夹中存放了一些主题、字体、样式与图像等文件 预览 $ gitbook serve ./{book_name} 最后一个参数指定输出静态网站内容的目录，可省略，默认会在当前目录下新建一个子目录_book (base) dantesu@DanteSudeMacBook-Pro gitbook % gitbook serve Live reload server started on port: 35729 Press CTRL+C to quit ... info: 25 plugins are installed info: 14 explicitly listed info: loading plugin \"splitter\"... OK info: loading plugin \"expandable-chapters-small\"... OK info: loading plugin \"anchors\"... OK info: loading plugin \"github\"... OK info: loading plugin \"github-buttons\"... OK info: loading plugin \"sharing-plus\"... OK info: loading plugin \"anchor-navigation-ex\"... OK info: loading plugin \"favicon\"... OK info: loading plugin \"livereload\"... OK info: loading plugin \"highlight\"... OK info: loading plugin \"search\"... OK info: loading plugin \"lunr\"... OK info: loading plugin \"fontsettings\"... OK info: loading plugin \"theme-default\"... OK info: found 10 pages info: found 2 asset files info: >> generation finished with success in 1.1s ! Starting server ... Serving book on http://localhost:4000 样式 $ gitbook install 更新样式中的插件后需要使用此命令来安装新的插件，否则会报错，比如： (base) dantesu@DanteSudeMBP gitbook % gitbook serve Live reload server started on port: 35729 Press CTRL+C to quit ... info: 25 plugins are installed info: 16 explicitly listed Error: Couldn't locate plugins \"page-footer-ex\", Run 'gitbook install' to install plugins from registry. 样式例子 以下是我暂时在使用的样式 { \"title\": \"DanteSU's House\", \"author\": \"DanteSU\", \"description\": \"All I know is here\", \"language\": \"zh-hans\", \"gitbook\": \"3.2.3\", \"styles\": { \"website\": \"./styles/website.css\" }, \"structure\": { \"readme\": \"README.md\" }, \"links\": { \"sidebar\": { \"但丁世界（在建）\": \"https://dante-su.github.io/\" } }, \"plugins\": [ \"-sharing\", \"splitter\", \"expandable-chapters-small\", \"anchors\", \"github\", \"github-buttons\", \"sharing-plus\", \"anchor-navigation-ex\", \"favicon\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/Dante-Su\" }, \"sharing\": { \"douban\": false, \"facebook\": false, \"google\": false, \"hatenaBookmark\": false, \"instapaper\": false, \"line\": false, \"linkedin\": false, \"messenger\": false, \"pocket\": false, \"qq\": false, \"qzone\": false, \"stumbleupon\": false, \"twitter\": false, \"viber\": false, \"vk\": false, \"weibo\": false, \"whatsapp\": false, \"all\": [ \"google\", \"facebook\", \"weibo\", \"twitter\", \"qq\", \"qzone\", \"linkedin\", \"pocket\" ] }, \"anchor-navigation-ex\": { \"showLevel\": false }, \"favicon\":{ \"shortcut\": \"./source/images/favicon.jpg\", \"bookmark\": \"./source/images/favicon.jpg\", \"appleTouch\": \"./source/images/apple-touch-icon.jpg\", \"appleTouchMore\": { \"120x120\": \"./source/images/apple-touch-icon.jpg\", \"180x180\": \"./source/images/apple-touch-icon.jpg\" } } } } 样式网站 https://www.npmjs.com/search?q=gitbook-plugin-theme&ranking=quality 部署到Github 创建新的book 相关链接 DanteSU            updated 2022-06-05 10:35:09 "},"OpCode/5.html":{"url":"OpCode/5.html","title":"Markdown","keywords":"","body":"Markdown Author = DanteSU 链接 页内链接 利用html标签实现 定义一个锚(id)： 跳转到的地方 跳转链接的位置： [点击跳转](#jump) 资源链接 超链接 表格 markdown方法 一般情况 | 表头1 | 表头2 | 表头3 | | :- | :-: | -: | | 信息1 | 信息2 | 信息3 | 效果： 表头1 表头2 表头3 信息11111111 信息22222222 信息33333333 格内换行 | 表头1 | 表头2 | | :-: | :-: | | 信息1 | 信息2信息3 | 效果： 表头1 表头2 信息1 信息2信息3 html方法 一般情况 表头1 表头2 单元格信息1 单元格信息2-1 单元格信息2-2 效果： 表头1 表头2 单元格信息1 单元格信息2-1 单元格信息2-2 更改对齐方式 表头1 表头2 表头3 单元格信息1 单元格信息2 单元格信息3 效果： 表头1 表头2 表头3 单元格信息1 单元格信息2 单元格信息3 表头 我是表头 效果： 我是表头 一格多行 表头1 表头2 单元格信息1 单元格信息2-1 单元格信息2-2 效果： 表头1 表头2 单元格信息1 单元格信息2-1 单元格信息2-2 一格多列 表头1 表头2 表头3 单元格信息1 单元格信息2 效果： 表头1 表头2 表头3 单元格信息1 单元格信息2 数学公式 DanteSU            updated 2022-06-05 10:51:20 "},"OpCode/6.html":{"url":"OpCode/6.html","title":"Others","keywords":"","body":"Others Author = DanteSU Colab防断 https://colab.research.google.com/github/iErics/gd-utils/blob/master/Colab_gd_utils.ipynb function ConnectButton(){ console.log(\"Connect pushed\"); document.querySelector(\"#connect\").click() } setInterval(ConnectButton,60000); python文件中多行平移 左移 选中+按下Tab 右移 选中+按下ctrl+[ Mac是按下Command+[ clear清屏幕出现问题 问题描述 $ clear terminals database is inaccessible 解决方法 $ export TERMINFO=/usr/share/terminfo 最好是将上面那条 export 命令添加到 .bashrc 中。 Under building... DanteSU            updated 2022-06-05 11:43:37 "},"Mix/":{"url":"Mix/","title":"杂记","keywords":"","body":"杂记 笔记 bug经验集 bug解决链接 岗位资源 及时代码 图像缩放 图像修改格式 PyTorch DanteSU            updated 2022-06-05 11:29:21 "},"Mix/1.html":{"url":"Mix/1.html","title":"笔记","keywords":"","body":"笔记 Author = DanteSU 计算机视觉 医学图像处理 1 From 知乎 划重点! ! ! 其实医学领域的顶会和顶刊相对来说好中一点!只要大家多从CVPR/ICCV/ECCV.上看一些相关论文，从里面跟踪一些最新技术(创新点)，再把这些新技术(创新点)应用到医学图像领域的某些任务,打败该任务下现有的SOTA算法并解决相关的问题即可。0K~，恭喜你可以开始写论文了!至于中不中还要看你写的怎么样，毕竟顶会顶刊的投稿质量可不允许错误语法一大堆， 图表不规范，以及文章逻辑不通等! English TOEFL Duolinguo English Test (DET) Supervisors with high quality publications in HK 1 From 知乎 作者：lenn 链接：https://www.zhihu.com/question/332075078/answer/738266074 CUHK: IE/EE: Multimedia Laboratory 全部faculty (http://mmlab.ie.cuhk.edu.hk/) CSE: Prof. Jiaya Jia (http://jiaya.me/) Dr. Qi Dou (http://www.cse.cuhk.edu.hk/~qdou/) HKUST: CSE: Prof. Dit-Yan Yeung (https://sites.google.com/view/dyyeung) Prof. James Kwok (https://www.cse.ust.hk/~jamesk/) Chi Keung Tang (http://www.cs.ust.hk/~cktang/bio-sketch-review.htm) Prof. Long Quan (https://www.cse.ust.hk/~quan/) Prof. Pedro V. Sander (https://www.cse.ust.hk/~psander/) Dr. Qifeng Chen (https://cqf.io/) Dr. Dan Xu (Dan Xu - Homepage) ECE: Dr. Shaojie Shen (https://www.ece.ust.hk/eeshaojie) HKU: CS: Prof. Yizhou Yu (https://i.cs.hku.hk/~yzyu/) Dr. Ping Luo (http://luoping.me/) EE: Dr. Xiaojuan Qi (Xiaojuan Qi) CityU: CS: Prof. Chong-Wah Ngo (http://vireo.cs.cityu.edu.hk/index.html) Dr. Antoni Bert Chan (http://visal.cs.cityu.edu.hk/) Prof. Rynson Lau (http://www.cs.cityu.edu.hk/~rynson/) Dr. Kede Ma (Kede Ma, home page) Dr. Jing Liao (https://liaojing.github.io/html/) SCM: Prof. Hongbo Fu (http://sweb.cityu.edu.hk/hongbofu/) PolyU: COMP: Prof. Lei Zhang (https://www4.comp.polyu.edu.hk/~cslzhang/) Dr. Bo Yang (The Hong Kong Polytechnic University) HKBU: CS: Prof. Yiu Ming Cheung (https://www.comp.hkbu.edu.hk/~ymc/) 进程与线程 From 知乎 类似”进程是资源分配的最小单位，线程是CPU调度的最小单位“这样的回答感觉太抽象，都不太容易让人理解。 做个简单的比喻：进程=火车，线程=车厢 线程在进程下行进（单纯的车厢无法运行） 一个进程可以包含多个线程（一辆火车可以有多个车厢） 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘） 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易） 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源） 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢） 进程可以拓展到多机，进程最多适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上） 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－\"互斥锁\" 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量” 散记 1 简化版Attention矩阵跟标准的Scaled-Dot Self Attention类似，这里的注意力矩阵还是Q,K的内积并除以维度的平方根而来，复杂度还是𝒪(n2)的，不同的是这里简化了Q,K的来源变换，并且激活函数换用了relu2。大家可能对这个激活函数比较陌生，事实上这是作者团队在他们之前的论文《Primer: Searching for Efficient Transformers for Language Modeling》用NAS的方式搜出来的。最后的1/n是简单的归一化因子，用以消除长度的影响。这个设计的成功也表明，注意力机制中的softmax不是必须的，可以换成常规的激活函数加简单的归一化。 DanteSU            updated 2022-06-05 20:40:34 "},"Mix/2.html":{"url":"Mix/2.html","title":"bug经验集","keywords":"","body":"bug经验集 Author = DanteSU linux服务器本地开启visdom 需要用pytorch的visdom在服务器上跑程序，并把图片显示出来，但是服务器上打不开网页，如何将visdom.server转到本地。以下操作 1.在服务器上使用 $ tmux new -s session-name 开启一个新的后台进程，也可以直接在前台开启visdom.server，只不过就没办法进行其他工作。 2.开启visdom所在conda虚拟环境，输入 $ python -m visdom.server 开启visdom服务器 3.在本地，注意是本地电脑，而非服务器命令行，打开cmd，输入 $ ssh -L 8097:127.0.0.1:8097 username@xx.xx.xx.xx （username和xx.xx.xx.xx分别是你服务器的用户名和IP地址） 4.你会发现本地cmd命令行连接上了服务器，然后你就可以在浏览器中打开127.0.0.1:8097 发现连接上了 5.如果想终止服务器和本地的连接，可以杀死进程即可 $ tmux kill-session -t session-name DanteSU            updated 2022-06-05 23:01:41 "},"Mix/3.html":{"url":"Mix/3.html","title":"bug解决链接","keywords":"","body":"bug解决链接 Searcher = DanteSU Mirror 名称 网址 清华源 https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/linux-64/ DP 名称 网址 pytorch 加载大数据集 内存不够 的处理方式 https://blog.csdn.net/cjs8348797/article/details/115708811https://www.cnblogs.com/aminor/p/14336767.htmlhttps://www.zhihu.com/question/386743819/answer/1989311050https://www.cnblogs.com/xiaosongshine/p/10750908.html AdaptivePooling与Max/AvgPooling相互转换 https://www.cnblogs.com/xiaosongshine/p/10750908.html Python中生成并绘制混淆矩阵 https://blog.csdn.net/kane7csdn/article/details/83756583 Pytorch 完整的模型训练套路 https://blog.csdn.net/weixin_45468845/article/details/122971739https://zhuanlan.zhihu.com/p/464796719https://blog.csdn.net/FUTEROX/article/details/122724634 pytorch中根据神经网络结构确定输入图片尺寸/根据图片尺寸修改神经网络结构 https://blog.csdn.net/weixin_43423455/article/details/99096580 torch.nn.AdaptiveAvgPool1d(N)函数解读 https://blog.csdn.net/qq_40178291/article/details/102699493 torch.view()详解及-1参数是什么意思 https://www.cnblogs.com/MartinLwx/p/10543604.html Pytorch常用的交叉熵损失函数CrossEntropyLoss()详解 https://zhuanlan.zhihu.com/p/98785902 PyTorch 多分类损失函数 https://blog.csdn.net/jacke121/article/details/104665912/ pytorch的各种loss https://blog.csdn.net/qq_22764813/article/details/104867431 PyTorch Bert文本分类 https://blog.csdn.net/weixin_44912902/article/details/123886825 二分类问题：基于BERT的文本分类实践！附完整代码 https://blog.csdn.net/Datawhale/article/details/104871803?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-104871803-blog-123886825.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-104871803-blog-123886825.pc_relevant_default&utm_relevant_index=6 BERT-使用tf生成pytorch_model.bin https://www.freesion.com/article/9725381185/ PyTorch之nn.ReLU与F.ReLU的区别 https://blog.csdn.net/u011501388/article/details/86602275 Pytorch入门教程（十）：ResNet图片分类实战 https://blog.csdn.net/weixin_43472830/article/details/95871258?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-95871258-blog-107028785.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-95871258-blog-107028785.pc_relevant_default&utm_relevant_index=5 pytorch版yolov3训练自己数据集 https://www.cnblogs.com/pprp/p/10863496.htmlhttps://github.com/BobLiu20/YOLOv3_PyTorchhttps://github.com/cxjaicj/keras-yolo3https://github.com/pprp/yolov3.keras ImportError:无法从“tensorflow”导入名称“Session” https://www.cnpython.com/qa/1297701 Adam优化算法详细解析 https://blog.csdn.net/luoxuexiong/article/details/90412213https://www.cnblogs.com/wuchengze/p/13610500.html 理解语言的 Transformer 模型 https://tensorflow.google.cn/tutorials/text/transformer papers 名称 网址 Vision Transformer 必读系列之图像分类综述(一)：概述 https://zhuanlan.zhihu.com/p/459828118 面向多场景低资源加密流量分类的加密流量预训练技术 https://zhuanlan.zhihu.com/p/483285843 跨模态检索 Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval https://zhuanlan.zhihu.com/p/398898919 跨模态检索论文解读 Similarity Reasoning and Filtration for Image-Text Matching https://zhuanlan.zhihu.com/p/394203914 论文笔记 FPN —— 特征金字塔 https://zhuanlan.zhihu.com/p/92005927 中国医学影像 AI 的 20 年「大变局」万字长文 https://baijiahao.baidu.com/s?id=1722558686425634791&wfr=spider&for=pc Others 名称 网址 解决 Failed to connect to github.com port 443:connection timed out https://blog.csdn.net/Hodors/article/details/103226958 python工具-将视频按帧截取图片（附代码） https://blog.csdn.net/qq_36190978/article/details/85284484 python中sorted是什么 https://m.php.cn/article/423733.html Python3创建目录mkdir https://blog.csdn.net/weixin_51697369/article/details/119864944 python enumerate用法总结 https://blog.csdn.net/churximi/article/details/51648388 python的文件操作 https://blog.csdn.net/zhandar44/article/details/95995091 词袋模型 https://github.com/gurkandemir/Bag-of-Visual-Wordshttps://github.com/cleanlii/bow-image-retrieval Standford cv reports 2017 http://cs231n.stanford.edu/reports/2017/pdfs/ 机器学习性能度量指标precision、recall、PR曲线、F1、ROC、AUC、IOU、mAP） https://blog.csdn.net/xhj_enen/article/details/88639920 Transformer 模型详解 https://baijiahao.baidu.com/s?id=1651219987457222196&wfr=spider&for=pc A Step by Step Backpropagation Example https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ Math for Computer Graphics https://faculty.cc.gatech.edu/~turk/math_gr_new.html 用Python读取CSV文件的5种方式 https://blog.csdn.net/qq_40907977/article/details/108054088 Shell脚本编写 https://blog.csdn.net/weixin_35784267/article/details/116582780 markdown表格 https://www.runoob.com/markdown/md-table.html DanteSU            updated 2022-06-05 23:01:44 "},"Mix/4.html":{"url":"Mix/4.html","title":"岗位资源","keywords":"","body":"岗位资源 NOTICE: 在每一个分隔栏之间，按照时间的顺序，最新的消息都在最上方易于寻找 Job Intern 华为云 Release Time: 5/31 2022 华为云AI算法实习生 （内推、咨询可加微信：zhixing1055321398） 工作内容： 负责华为云语自然语言处理、语音处理、计算机视觉、多模态学习等领域的算法及平台研发，包括但不限于智能问答、知识图谱、预训练大模型、语音合成、数字人构建、多模态预训练等领域相关模型的构建、优化、论文发表和算法落地等工作。 技能要求： 扎实的算法和编程基础，熟悉机器学习基本理论； 具有自然语言处理领域、语音或计算机视觉相关知识和技能，至少在其中一个子领域有较深的理解和经验，在高水平国际会议期刊发表过论文者优先； 熟练掌握至少一种编程语言（包括但不限于Java，Python)，熟悉Tensorflow、Pytorch等开发框架者优先。 工作地点：杭州，深圳，西安，北京 洛凯科技 Release Time: 5/28 2022 我们就是洛凯科技（Rockey Tech），一家专注海外社交的独角兽企业🚀 【高薪产品/运营实习岗，日薪200起】【宣讲送iphone13、switch的公司】 岗位招满即止，base：北京；招募范围：23-25届毕业生；【内推码】CDEJ7V1 网申链接：https://rockey.jobs.feishu.cn/index/m/?spread=MKX77B9 部门急招岗位待遇更好，优先筛选，免笔试，助你更快拿offer❗️ 🆘 游戏社区产品运营实习生（200~300元/天） 负责平台游戏库的更新和维护, 新游推荐； 负责社区用户运营, 维持用户活跃及回流; 负责社区内容运营, 确保产能与内容质量符合需求； 能够独立进行数据分析。 🆘 内容增长运营实习生(seo方向) （300~400元/天） 负责项目SEO海外增长业务; SEO流量监控与数据分析，通过数据变化调整优化策略，风险预警； 执行海外网站内容优化，通过内容质量提高SEO排名，优化用户体验； 管理搜索引擎相关广告账户，通过账户表现调整账户投放方向。 🆘 游戏编辑实习生（200~300元/天） 了解游戏玩家的不同风格、喜爱的游戏类型,富有同理心； 对游戏感兴趣，未来致力于从事游戏行业的同学； 负责公司平台的游戏推荐和测评。 🆘 游戏社区产品经理实习生 （300~400元/天） 负责游戏社区产品功能策划和迭代，提高留存和用户活跃度； 基于数据分析用户特征，挖掘用户需求，设计有效的产品方案，对转化、留存、召回等各环节指标负责; 配合运营及业务需求，负责平台资源分配策略优化，提升资源分发效率。 🆘 内容运营实习生（300~400元/天） 负责贴纸产品内容运营，包括日常内容更新、优化、品类测试; 关注用户反馈，针对用户反馈提出可优化的产品建议； 配合完成产品投放，针对投放地区进行素材优化。 广联达 Release Time: 5/21 2022 广联达秋招提前批&暑期实习内推 🌟本次提前批不影响秋招正式投递；无违约金，别人还在找实习，你却已经拿了保底offer； 🌟双休制，公积金满额缴纳 Base: 全国多地可选 ✔️提前批岗位：C++开发工程师、Java开发工程师、前端开发工程师、图形开发工程师、 图形 算法开发工程师、人工智能工程师、 测试开发工程师、产品经理 ✔️暑期实习岗位：销售工程师、服务工程师、实施工程 网申链接👇 https://app.mokahr.com/m/campus_apply/glodon/1750?recommendCode=DSnskg8B#/jobs 内推码👉DSnskg8B [爱心]广联达2023届校园招聘来啦！[爱心] [庆祝]广联达科技股份有限公司成立于1998 年，长期立足建筑产业，围绕建设工程项目的全生命周期，是提供以建设工程领域专业应用为核心基础支撑，以产业大数据、产业新金融为增值服务的数字建筑平台服务商。广联达经过 20 余年的发展，在全球建立 80 余家分子公司，拥有员工8000余人，涵盖全产业链包含设计、造价、施工、运维、供采、园区，以及金融、教育、投资并购等业务，为30余万企业用户，百万专业工程技术和管理人员，提供近百款专业应用产品及服务。 [庆祝]面向2023年应届生，本科及以上学历，硕士、博士 优先；计算机、数学、机械、土木、建筑、GIS、仿真、人工智能/自动化、软件工程等理工科相关专业优先，其他专业不限； [庆祝]工作地：TOT研发序列bsae北京、上海、西安、广州，EOE销服序列base全国（具体岗位工作城市可在 投递时查看） [庆祝]岗位薪酬：TOT研发岗位30W+，EOE销服岗位15W+ 稳定舒适的办公环境，上市公司大平台，福利待遇好，横向纵向多种发展通道； 七险一金（包括商业意外险+补充医疗），双休，法定节假日及带薪年假； 节日福利（法定节假日及妇女节、五四青年节、母亲节、儿童节、圣诞节等）生日礼品惊喜，月度和季度奖金； 午餐补助、交通和通信补助，高温补贴、年终奖等； 每年定期免费体检，保证员工身体健康； 年度国内、外旅游团建。 [庆祝]加入我们： 【内推码】DSnskg8B PC 端网申通道：http://campus.glodon.com 移动端网申通道：关注“广联达招聘”公众号 集团官网：www.glodon.com 联系电话：18071552535（黄），15007120648（朱）（微信同号） 总部地址：中国-北京-海淀区西北旺东路10号院东区13号楼广联达信息大厦 湖北分公司地址：武汉市武昌区徐东大街群星城K3-2座20楼 图森未来 Release Time: 5/08 2022 【高薪外企内推，图森未来】【应届/实习】 🌟解决北京、上海户口/七险一金/双休/公积金缴纳12%/实习提供免费住房/实习月薪8k起/算法岗8卡2080Ti标配… 🔥无人驾驶领域全球首家上市公司，面向全球提供可大规模商业化运营的无人驾驶卡车技术，并启动全球首个无人驾驶货运网络。 网申链接👇 https://app.mokahr.com/m/campus_apply/tusenweilai/35932?recommendCode=DSK8Wgdv#/jobs 内推码👉DSK8Wgdv 工作地点：北京、上海、国外 青藤云 Release Time: 4/26 2022 青藤云安全-信息安全实习生招聘（6月到岗即可） 【工作地点】 北京市海淀区创业路8号群英科技园 【工作职责】 python与shell恶意脚本检测引擎的开发； 恶意脚本检测引擎的测试； 搜集恶意脚本以及业界的检测方式； 【任职资格】 本科或以上学历； 熟悉python、shell和c语言，使用python或c做过实际项目； 了解编译原理，熟悉脚本语言内部执行过程； 对网络安全知识有一定了解，熟悉常见的脚本利用漏洞； 【工作时长】 9：30—19：00 【福利待遇】 实习补贴180-200/天，有餐补、房补、电脑补贴，可开具实习证明，团队氛围nice 【申请方式】 将简历发送至 hanyl1104@163.com，简历及邮件命名格式为【姓名+学校+专业】 有问题加wx 13641251014 OPPO Release Time: 4/14 2022 OPPO22届春招｜23届暑期实习生火热开启！投递使用内推码优先筛选! 内推码：XYDS42780811 网申网址：https://careers.oppo.com/campus 软件类、硬件类、AI/算法类、产品类、工程技术类、采购类、品牌策划类、销售服务类、综合职能类 总有一款适合你 OPPO工作环境VR上线，欢迎沉浸体验：https://720yun.com/t/38vkz9fwgpl#scene_id=78676648 2022届春招缺口较多岗位(依次排序)：系统工程师，安卓应用工程师，驱动工程师，无线通信协议工程师，Linux系统工程师，多媒体开发工程师，计算机视觉算法开发工程师 实习生岗位缺口： 非技术类缺口较多的岗位：财务管理专员，关务管理专员 技术类缺口较多的岗位：计算机视觉算法，影像算法，linux系统，安卓应用(缺口最大)，多媒体开发，多媒体系统，驱动，系统工程师(缺口次多)，软件测试开发(缺口第三多) 内推码XYDS42780811 OPPO华科官方校招群 913802534 百度 Release Time: 4/13 2022 百度23届实习内推 ，即将截止 🌟 600+OFFER 发放、多种岗位选择，实习转正率超过70% 🌟 内推简历免筛选 ，直达笔试 地点：北京、上海、深圳、大连 网申👉 https://talent.baidu.com/ 内推码👉ISKWSB ㊙️秋招提前批7月开启，内推免笔试， 直通面试～没办法实习的同学，记得收藏内推码，秋招提前批助你更快拿 offer❗ 深信服 Release Time: 4/10 2022 深信服实习生招聘 岗位：开发类、市场类、安全类、算法类、产品规划类等六⼤类岗位 地点：深圳、⻓沙、北京、南京 福利：包住宿、包三餐、包往返车票、月薪4000起，更多福利等你解锁 投递通道： pc端：https://app.mokahr.com/campus_apply/sangfor/6146#/home ⼿机端：关注【深信服招聘】-【校园招聘】-【实习招聘】 信服圈（为了更好的通过面试，建议大家关注信服圈儿进行学习）： 人群： 对简历如何书写、面试如何准备有相关疑惑的 已经明确了自己的求职方向，想要提升能力的 想要和更多志同道合的人一起交流求职想法的 不确定自己未来职业发展方向该如何做抉择的 内容：体系化的市场课程、hr 模拟面试、研发校招真题、学长学姐爆料等 福利：免费的简历模板，专业化的hr辅导，海量的行业知识等 如何关注： 扫码或直接微信公众号搜索【信服圈⼉】进行关注 更多详情：https://mp.weixin.qq.com/s/RIRirwgM72dqCeBgloNlLg 北京泰铼投资 Release Time: 4/2 2022 春季校招/实习 量化研究员/交易员/市场/数据开发...多岗位招募中📮 时间：实习岗位需实习满4个月及以上，校招岗需毕业前实习[太阳] 工作地点：北京西城区西环广场 工作内容: 涉及量化投研、二级市场交易、金融机构对接合作、交易系统开发等多种方向 希望你: 2022届或2023届国内外应届毕业生 有数学、计算机、金融等相关专业背景，专业基础扎实，逻辑清晰，表达清楚 有量化投资或互联网算法/开发相关实习经验的同学优先 薪酬福利： 研究实习生RMB 400~600/天， 研发实习生RMB 300/天，校招薪资单独沟通，wlb拒绝996 表现优秀的实习生将被推荐参与之后的项目，并可获得转正机会 公司正式实习生，可开实习证明 内推码：DS5bJZPu 腾讯 Release Time: 2/18 2022 🔥 腾讯2022实习生招聘即将启动！ 欢迎大家搜索/扫码加入【腾讯华中区23届官方实习交流群】 群内将会提供： 腾讯官方招聘信息 解答关于实习的各种疑问 线上线下活动预告 ✨ 技术类QQ群群号：701302439 ✨ 非技术类QQ群群号：760545855 Research Intern Dr. Yang YOU in NUS Release Time: 4/20 2022 新加坡国立大学(NUS) AI高性能计算实验室实习生招募，导师为尤洋老师，尤老师博士毕业于加州大学伯克利分校，尤老师个人主页：https://www.comp.nus.edu.sg/~youy/，实验室主页：https://ai.comp.nus.edu.sg/ 本次CVPR 2022实验室投稿5篇论文全部被接收： https://www.zhihu.com/question/502566228/answer/2379895566。 尤老师是NUS的校长青年教授，实验室氛围开放，加入的话有发过顶会的学长亲自指导，可供选择的方向包括自监督、数据集治理、自动驾驶、图像检索、度量学习等，实习结束后可以提供在NUS的科研经历证明。在国内的同学可以推荐到阿里、字节、美团、华为等公司纯科研实习，有条件的同学也可以申请经费到新加坡交流。感兴趣的同学可以发邮件到kai.wang@comp.nus.edu.sg VSRP of KAUST Release Time: 长期有效 阿卜杜拉国王科技大学的 VSRP(Visiting Student Research Program) 项目，全球相关研究领域本科生、硕士生、博士生均可申请，每月工资 1000 USD，办公提供 Mac，可以解决出行问题，线下参加需要两针得到认证的疫苗接种证明，在COVID-19席卷全球期间可以申请Remote。 具体项目与监督者详见网址：https://vsrp.kaust.edu.sa PhD Position Dr.Lu Cheng in UIC Release Time: 5/5 2022 华科学姐【招生】伊利诺伊芝加哥分校（UIC）- 计算机系 - 博士/硕士/实习生 - 负责任人工智能/数据挖掘/因果机器学习/社交媒体挖掘 (2022秋，2023春/秋) https://www.1point3acres.com/bbs/thread-892043-1-1.html 伊利诺伊大学芝加哥分校（the University of Illinois at Chicago 简称 UIC）由原伊利诺伊大学医学中心与位于芝加哥的分校于1982年合并而成是美国国家资助的公立研究型大学以及伊利诺伊大学系统的第二个成员实力仅次于伊利诺伊大学厄巴纳-香槟分校；同时该校也是由卡内基基金会评选的88所研究型一类美国大学以及全美最大的10所大学之一作为伊利诺伊大学系统三所分校之一UIC位于美国伊利诺伊州芝加哥市中心是一所著名的公立研究型大学同时也是芝加哥地区规模最大、综合实力最强的公立大学学校共分为东、西、南三个校区计算机系位于东校区也是最安全的校区UIC计算机系2022年USNews全美排名60芝加哥具有优越的海滨地理环境以及许多高档餐饮场所芝加哥市中心是芝加哥最重要的区域也是该城市聚集金融、文化、政府和商业机构的中心 UIC计算机系是一个以研究为主的院系目前招收多名博士生所有录取博士均有奖学金（TA/RA）支持在PhD前两年所有学生都有TA机会不需要导师funding支持可以自由选择导师UIC计算机系在数据挖掘和自然语言处理领域都有非常好的成就关于我个人信息就用英文了 Fully funded PhD positions are available in Dr.LuCheng's lab (http://wwwpublicasuedu/~lcheng35/) in the Department of Computer Science at the University of Illinois at Chicago. The main research direction will be socially responsible AI (fairness, interpretability/explainability,privacy,robustness,domain generalization) causal machine learning and social media mining. Highly motivated master's students/undergraduate/interns are also very welcome to work with me. Dr.Lu Cheng received her Ph.D. degree in Computer Science at Arizona State University(ASU) in 2022 advised by Dr.Huan Liu, M.Eng. degree in Industrial Engineering at Rensselaer Polytechnique Institute(RPI) and B.Eng. degree in Industrial Engineering at Huazhong University of Science & Technology(HUST). Her research interests are broadly in data mining and machine learning, with aparticular focus on socially responsible AI, social computing, causal machine learning and AI for social good. Lu is the web chair of WSDM'22 and a senior program committee member of AAAI'22. She was the recipient of the 2021 ASU Engineering Dean's Dissertation Award, 2020 ASU Graduate Outstanding Research Award, 2021 and 2022 ASU CIDSE Doctoral Fellowship, 2022 SDM Best Poster Award, 2019 ASU Grace Hopper Celebration Scholarship, IBM Ph.D. Social Good Fellowship, Visa Research Scholarship and various Student Travel Awards and Scholarships. More information can be found at http://wwwpublicasuedu/lcheng35/ Requirement: A bachelor's degree in Computer Science Computer Engineering Electrical Engineering or related fields Experience in at least one major programming language Capabilities and willingness to learn new programming languages Enthusiasm for doing research Students who are interested in joining my lab as PhD students in Fall 2022 need to be US-based(Not applied to other semesters) Application Process: Please send your CV and transcript to luchengrecuit@gmailcom 由于最近deadline比较多 回复邮件可能比较慢 望见谅！ Dr. Kai ZHAO in UAB Release Time: 4/13 2022 发个招生信息，还请大家多多支持[机智] 阿拉巴马大学伯明翰分校(The University of Alabama at Birmingham) 助理教授赵恺 ( https://cs.ucr.edu/~kzhao016/ ) 招收2022秋季或2023春季入学的计算机博士生3~4名。教授本人本科毕业于北京大学计算机系，博士毕业于加州大学河滨分校计算机科学与工程系。研究方向覆盖高性能计算，并行分布式计算，大数据的管理和分析，高能效深度学习等。赵恺教授在数据库和高性能计算两个领域的顶级会议(ICDE, HPDC, SC, ICS)发表论文十余篇，所参与研发的数据压缩框架SZ获得了2021年的美国R&D100创新奖(2021 R&D 100 Awards)。 学生资助和发展：所有录取学生均提供全额奖学金(包括生活费，学费，和健康保险)并提供实验设备(最新Mac或Linux电脑)。赵恺教授和美国阿贡国家实验室(Argonne National Laboratory)保持着长期合作，学生表现优异可在暑假前往国家实验室跟随各个领域的资深科学家实习。 大学简介：阿拉巴马大学伯明翰分校(简称UAB)，是一所美国著名的公立研究型大学，拥有极高的全球排名(US News 第147，THE 第169)。UAB医学系统是美国最大的医学中心之一，为学生提供无忧的健康保障。UAB计算机科学系研究领域覆盖高性能计算，并行和分布式计算，数据挖掘，医疗数据分析，机器学习，系统和网络安全，区块链，云计算，编程语言等。计算机科学系于2019年搬入全新的办公楼，拥有领先的硬件设施(例如全M1的Mac教室)。UAB城郊(Vestavia Hills等地)距离学校10分钟车程，治安远好于美国绝大多数地区，并拥有完善的商业(Costco, 陈家园 Hometown Supermarket, Trader Joe's等)，十分适合亚裔生活。UAB交通便利，距离伯明翰机场10分钟，距离亚特兰大，纳什维尔，和孟菲斯车程2-3小时，距离墨西哥湾最佳白沙滩 Destin 和大雾山国家公园车程4小时，周末休闲采购娱乐都相对便捷。 学生要求：本科就读任何专业的学生均可申请，不要求有科研经历。要求学生自学能力强，逻辑思维严密，有恒心和毅力，并对攻读博士学位有强烈兴趣。 申请方式：请将简历，TOEFL成绩，课程成绩单，及个人专长等发送到kzhao@uab.edu。 Dr. Peng GAO in Virginia Tech Release Time: 3/7 2022 18届王xx：和高教授做了六个月intern，有了两篇很好的产出，申请季高教授给了我巨大的支持和帮助，和我单独聊了几个小时分享他读博的经验建议。最后因为个人原因选了地理位置在加州的学校，没有继续留组读PhD。非常推荐高教授的团队，教授非常nice和friendly，科研质量很高，工业界学术界connection都非常强。推荐给在找暑研和22Fall/23Spring/23Fall PhD机会的朋友们 https://people.cs.vt.edu/penggao/ DanteSU            updated 2022-06-05 21:37:20 "},"CodeForUse/":{"url":"CodeForUse/","title":"及时代码","keywords":"","body":"图像处理 图像缩放 图像修改格式 PyTorch DanteSU            updated 2022-06-05 21:14:58 "},"CodeForUse/1.html":{"url":"CodeForUse/1.html","title":"图像缩放","keywords":"","body":"图像缩放 Author = DanteSU import cv2 import os import glob def img_resize(): # iterations path = input(r\"Input the path of image to be processed(eg: D:\\picture\\1.jpg):\") print('Path of image here is : ',path) path_rewrite = input(r\"Input the path of restoring the image(eg: D:\\picture):\") for i in glob.glob(path): print('I here is : ', i) im1 = cv2.imread(i) # print('The original image data are: ', im1) im2 = cv2.resize(im1,(256,256)) # (256,256)是缩放后的像素数 # print('The resized image data are: ', im2) cv2.imwrite(os.path.join(path_rewrite,'resized_' + os.path.basename(i)),im2) if __name__ == '__main__': img_resize() DanteSU            updated 2022-06-05 10:56:21 "},"CodeForUse/2.html":{"url":"CodeForUse/2.html","title":"图像修改格式","keywords":"","body":"图像修改格式 Author = DanteSU 一般情况 import cv2 import os import glob def change_img_format(): # iterations path = input(r\"Input the path of image to be processed(eg: D:\\picture\\1.jpg):\") print('Path of image here is : ',path) path_rewrite = input(r\"Input the path of restoring the image(eg: D:\\picture):\") img_format = input(r\"Input the format you want(eg: jpg):\") for i in glob.glob(path): print('I here is : ', i) im = cv2.imread(i) new_path = os.path.join(path_rewrite,'new_name'+'.'+img_format) cv2.imwrite(new_path,im) if __name__ == '__main__': change_img_format() 针对 ico （图标）文件 ''' 常用图标大小： [ (256, 256), (128, 128), (64, 64), (48, 48), (32, 32), (24, 24), (16, 16) ] ''' from PIL import Image def make_ico_file(src_image_file, dist_ico_file, size): size = [int(size), int(size)] image = Image.open(src_image_file) image_cropped = image.crop((0, 0, 256, 256)) image_cropped.save(dist_ico_file, sizes=size) if __name__ == '__main__': make_ico_file(input(r\"Input the path of the image(eg: D:\\picture\\1.jpg):\"), input(r'Input the name of icon(eg: favicon):'), input(r'Input the same of icon(eg: 256):')) DanteSU            updated 2022-06-05 21:17:11 "},"CodeForUse/3.html":{"url":"CodeForUse/3.html","title":"PyTorch","keywords":"","body":"PyTorch Author = DanteSU 样例 train.py import time import click from datetime import datetime import torch # from torch import nn from torch.utils.data import DataLoader # from torch.utils.tensorboard import SummaryWriter # from sklearn.metrics import confusion_matrix from utils.loss import * from utils.model import * from utils.loader import * from utils.others import * def train(gpu_set, epoch, learning_rate, data_path, batch_size, loss, weight_decay='0.00001', model_name = 'dp_cnn', time_now = 'fake time'): # Load train and val data print('Loading data...') trainset, valset = load_train_data(data_path) # Get and print useful settings print('Getting settings...') train_data_size,val_data_size,n_classes,input_lenghth = get_setting(trainset, valset) # Using DataLoader to load data print('Setting dataloader...') train_dataloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True ) val_dataloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=True ) # Set pytorch device print('Setting device...') device = get_device_name(gpu_set) # Create the model print('Creating model network...') model = model_set(model_name,n_classes, batch_size,input_lenghth).to(device) # Creat loss function print('Creating loss function...') loss_fn = loss_set(loss,device) # create optimizer print('Creating optimizer...') # optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay, amsgrad=False) # set some sets total_train_step = 0 # record training's number # total_val_step = 0 # record validating's number # use tensorboard # writer = SummaryWriter(\"logs_train\") for i in range(epoch): epoch_start_time = iters_time = time.time() # timer for entire epoch # iters_time = time.time() print(\"------The {}th epoch starts------\".format(i+1)) # train starts training(train_dataloader,device,model,loss_fn,optimizer,batch_size,total_train_step,iters_time) validating(val_dataloader,val_data_size,device,model,loss_fn,time_now) save_checkpoints(time_now,model,model_name,i) print('The time consumption of this code is : {} s'.format(time.time() - epoch_start_time)) # writer.close() @click.command() @click.option('-g', '--gpu_set', type=str, default='-1', help='gpu_set: e.g. 0 0,1,2, 0,2. use -1 for CPU') @click.option('-dp', '--data_path', type=str, default='data', help='path of input data') @click.option('-e', '--epoch', type=int, default=10, help='epoch') @click.option('-bs', '--batch_size', type=int, default=1, help='batch size') @click.option('-lr', '--learning_rate', type=float, default=1e-3, help='batch size') @click.option('-l', '--loss', type=str, default='ce', help='loss function') @click.option('-wd', '--weight_decay', type=float, default=1e-5, help='weight decay') @click.option('-m', '--model_name', type=str, default='lstm', help='eg. lstm') def main(gpu_set, data_path, epoch, batch_size,learning_rate, loss, weight_decay,model_name): total_time = time.time() time_now = datetime.now() record_parameter(gpu_set, epoch = epoch, learning_rate = learning_rate, data_path=data_path, batch_size = batch_size, loss = loss, weight_decay = weight_decay, model_name = model_name, time_now = time_now) train(gpu_set, epoch = epoch, learning_rate = learning_rate, data_path=data_path, batch_size = batch_size, loss = loss, weight_decay = weight_decay, model_name = model_name, time_now = time_now) run_time = time.time() - total_time record_run_time(run_time) if __name__ == '__main__': main() utils loader.py import torch from torch.utils.data import Dataset import numpy as np import time def load_train_data(data_path): print('preparing trainset...') trainset_time = time.time() trainset = TrainDataset(data_path=data_path) print('trainset time consumption is :', time.time()-trainset_time) print('preparing valset...') valset_time = time.time() valset = ValDataset(data_path=data_path) print('trainset time consumption is :', time.time()-valset_time) return trainset, valset class TrainDataset(Dataset): \"\"\" 下载数据、初始化数据，都可以在这里完成 \"\"\" def __init__(self, data_path): self.X_train = self.npy_loader(data_path+'/x_train.npy') self.y_train_onehot = self.npy_loader(data_path+'/y_train_onehot.npy') def __getitem__(self, index): return self.X_train[index].unsqueeze(0), self.y_train_onehot[index] # .unsqueeze(0) def __len__(self): return len(self.X_train) def npy_loader(self,path): return torch.from_numpy(np.load(path)) class ValDataset(Dataset): \"\"\" 下载数据、初始化数据，都可以在这里完成 \"\"\" def __init__(self, data_path): self.X_val = self.npy_loader(data_path+'/x_val.npy') self.y_val_onehot = self.npy_loader(data_path+'/y_val_onehot.npy') def __getitem__(self, index): return self.X_val[index].unsqueeze(0), self.y_val_onehot[index] # .unsqueeze(0) def __len__(self): return len(self.X_val) def npy_loader(self,path): return torch.from_numpy(np.load(path)) class TestDataset(Dataset): \"\"\" 下载数据、初始化数据，都可以在这里完成 \"\"\" def __init__(self, data_path): self.X_test = self.npy_loader(data_path+'/x_test.npy') self.y_test_onehot = self.npy_loader(data_path+'/y_test_onehot.npy') def __getitem__(self, index): return self.X_test[index].unsqueeze(0), self.y_test_onehot[index] # .unsqueeze(0) def __len__(self): return len(self.X_test) def npy_loader(self,path): return torch.from_numpy(np.load(path)) loss.py import sys import torch from torch import nn import torch.nn.functional as F from torch.autograd import Variable def loss_set(loss, device): if loss =='ce': return nn.CrossEntropyLoss() if loss == 'bce': return nn.BCELoss() if loss =='mlsm': return nn.MultiLabelSoftMarginLoss() if loss =='mlcce': return multilabel_categorical_crossentropy if loss =='fl1': return WeightedFocalLoss() if loss =='fl2': return focalBceLoss if loss == 'fl3': return focal_loss() if loss == 'fl4': return FocalLoss() else: print('loss name error!!!') return sys.exit(0) class WeightedFocalLoss(nn.Module): \"Non weighted version of Focal Loss\" def __init__(self, alpha=.25, gamma=2, device = 'cuda:2'): super(WeightedFocalLoss, self).__init__() self.alpha = torch.tensor([alpha, 1-alpha]).to(device) self.gamma = gamma def forward(self, inputs, targets): BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none') targets = targets.type(torch.long) at = self.alpha.gather(0, targets.data.view(-1)) pt = torch.exp(-BCE_loss) F_loss = at*(1-pt)**self.gamma * BCE_loss return F_loss.mean() class GHM_Loss(nn.Module): def __init__(self, bins, alpha): super(GHM_Loss, self).__init__() self._bins = bins self._alpha = alpha self._last_bin_count = None def _g2bin(self, g): return torch.floor(g * (self._bins - 0.0001)).long() def _custom_loss(self, x, target, weight): raise NotImplementedError def _custom_loss_grad(self, x, target): raise NotImplementedError def forward(self, x, target): g = torch.abs(self._custom_loss_grad(x, target)) bin_idx = self._g2bin(g) bin_count = torch.zeros((self._bins)) for i in range(self._bins): bin_count[i] = (bin_idx == i).sum().item() N = x.size(0) nonempty_bins = (bin_count > 0).sum().item() gd = bin_count * nonempty_bins gd = torch.clamp(gd, min=0.0001) beta = N / gd return self._custom_loss(x, target, beta[bin_idx]) class GHMC_Loss(GHM_Loss): def __init__(self, bins, alpha,device='cuda:2'): super(GHMC_Loss, self).__init__(bins, alpha) self.device = device def _custom_loss(self, x, target, weight): return torch.sum((torch.nn.NLLLoss(reduce=False)(torch.log(x),target)).mul(weight.to(self.device).detach()))/torch.sum(weight.to(self.device).detach()) def _custom_loss_grad(self, x, target): x=x.to(self.device).detach() target=target.to(self.device) return torch.tensor([x[i,target[i]] for i in range(target.shape[0])])-target class focal_loss(nn.Module): def __init__(self, alpha=0.25, gamma=2, num_classes = '11', size_average=True): \"\"\" focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi) 步骤详细的实现了 focal_loss损失函数. :param alpha: 阿尔法α,类别权重. 当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25 :param gamma: 伽马γ,难易样本调节参数. retainnet中设置为2 :param num_classes: 类别数量 :param size_average: 损失计算方式,默认取均值 \"\"\" super(focal_loss,self).__init__() self.size_average = size_average if isinstance(alpha,list): assert len(alpha)==num_classes # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重 print(\" --- Focal_loss alpha = {}, 将对每一类权重进行精细化赋值 --- \".format(alpha)) self.alpha = torch.Tensor(alpha) else: assert alpha2: input = input.view(input.size(0),input.size(1),-1) # N,C,H,W => N,C,H*W input = input.transpose(1,2) # N,C,H*W => N,H*W,C input = input.contiguous().view(-1,input.size(2)) # N,H*W,C => N*H*W,C target = target.view(-1,1) logpt = F.log_softmax(input) logpt = logpt.gather(1,target) logpt = logpt.view(-1) pt = Variable(logpt.data.exp()) if self.alpha is not None: if self.alpha.type()!=input.data.type(): self.alpha = self.alpha.type_as(input.data) at = self.alpha.gather(0,target.data.view(-1)) logpt = logpt * Variable(at) loss = -1 * (1-pt)**self.gamma * logpt if self.size_average: return loss.mean() else: return loss.sum() def focalBceLoss(pred,mask,device='cuda:3'): # pred[pred1]=1 # BCE_loss = F.binary_cross_entropy_with_logits(pred, mask) BCE_loss = multilabel_categorical_crossentropy(pred, mask) alpha = 0.25 alpha = torch.tensor([alpha, 1 - alpha]).to(device) gamma = 2 targets = BCE_loss.type(torch.long) at = alpha.gather(0, targets.data.view(-1)) pt = torch.exp(-BCE_loss) F_loss = at * (1 - pt) ** gamma * BCE_loss smooth = 1e-5 pred = torch.sigmoid(pred) num = mask.size(0) pred = pred.view(num, -1) #view()的作用相当于numpy中的reshape，重新定义矩阵的形状。 #print(mask.shape) mask = mask.contiguous().view(num, -1) intersection = (pred * mask) # dice = (2. * intersection.sum(1) + smooth) / (pred.sum(1) + mask.sum(1) + smooth) # dice = 1 - dice.sum() / num # print(type(dice)) # # 因为返回1维tensor，你可以想象成只含有1个元素的列表，要使用[0]索引才返回一个float数 # print(type(dice.item())) # print(type(F_loss.item())) # # print(dice) #print(type(F_loss.item())) return 0.5 * F_loss.item() + BCE_loss def multilabel_categorical_crossentropy(y_pred,y_true): y_pred = (1 - 2 * y_true) * y_pred y_pred_neg = y_pred - y_true * 1e12 y_pred_pos = y_pred - (1 - y_true) * 1e12 zeros = torch.zeros_like(y_pred[..., :1]) y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1) y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1) neg_loss = torch.logsumexp(y_pred_neg, dim=-1) pos_loss = torch.logsumexp(y_pred_pos, dim=-1) return neg_loss + pos_loss model.py import sys import math import torch from torch import nn, Tensor def model_set(model_name,n_classes, batch_size,input_lenghth): if model_name == 'resnet': return ResNet18(n_classes) if model_name == 'lstm': return LSTM(input_lenghth, n_classes) else: print('model name error!!!') return sys.exit(0) # model 1 class ResNet(nn.Module): def __init__(self, ch_in, ch_out): super(ResNet, self).__init__() self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(ch_out) self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(ch_out) self.relu = nn.ReLU(inplace=True) self.extra = nn.Sequential() # 如果输入、输出维度不同，需转化后才能相加 if ch_out != ch_in: self.extra = nn.Sequential( nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1), nn.BatchNorm2d(ch_out) ) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.extra(x) + out return out class ResNet18(nn.Module): def __init__(self,n_classes): super(ResNet18, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64) ) self.relu = nn.ReLU(inplace=True) # 4 block [b, 64, h, w] => [b, 1024, h, w] self.blk1 = ResNet(64, 128) self.blk2 = ResNet(128, 256) self.blk3 = ResNet(256, 512) self.blk4 = ResNet(512, 1024) # 注意最后全连接层维度，进去之前需要先打平 self.outlayer = nn.Linear(1024*30*50, n_classes) def forward(self, x): x = x.view(len(x),30,-1) x = x.unsqueeze(1) x = self.conv1(x) x = self.relu(x) x = self.blk1(x) x = self.blk2(x) x = self.blk3(x) x = self.blk4(x) x = x.view(x.size(0), -1) # 先打平，再进全连接 x = self.outlayer(x) return x # model 2 class LSTM(nn.Module): def __init__(self,INPUT_SIZE,n_classes): super(LSTM, self).__init__() self.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns input_size=INPUT_SIZE, hidden_size=64, # rnn hidden unit num_layers=1, # number of rnn layer batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size) ) self.out = nn.Linear(64, n_classes) def forward(self, x): # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state # choose r_out at the last time step out = self.out(r_out[:, -1, :]) return out other.py import torch import time from pathlib import Path def record_parameter(gpu_set, epoch, learning_rate, data_path, batch_size,loss, weight_decay, model_name, time_now): fp = open('log/log.txt','a') fp.write('-'*100 + '\\n' + 'new training starts :' + '\\n') fp.write('The time of running this code is {}:'.format(time_now) + '\\n') fp.write('The hyper parameters are as follows: \\n'+ 'using gpu ids : {} \\n'.format(gpu_set)+ 'the path of data : {} \\n'.format(data_path)+ 'epoch numbers : {} \\n'.format(epoch)+ 'batch size : {} \\n'.format(batch_size)+ 'learning rate : {} \\n'.format(learning_rate)+ 'loss function : {} \\n'.format(loss)+ 'weight decay : {} \\n'.format(weight_decay)+ 'model name : {} \\n'.format(model_name)) fp.close() def record_run_time(run_time): print('The time consumption of this code is : : {}'.format(run_time)) fp = open('log/log.txt','a') fp.write('The time consumption of this code is : {} \\n'.format(run_time)) fp.close() def get_setting(trainset, valset): # get useful settings train_data_size = len(trainset.X_train) val_data_size = len(valset.X_val) n_classes = len(trainset.y_train_onehot[0]) input_lenghth = len(trainset.X_train[0]) # print to see print(\"The length of train set is : {}\".format(train_data_size)) print(\"The length of val set is : {}\".format(val_data_size)) print(\"The number of classes is : {}\".format(n_classes)) print(\"The length of each input is : {}\".format(input_lenghth)) return train_data_size,val_data_size,n_classes,input_lenghth def get_device_name(gpu_set): if gpu_set == '-1': return torch.device('cpu') else: return torch.device('cuda:{}'.format(gpu_set)) def training(train_dataloader,device,model,loss_fn,optimizer,batch_size,total_train_step,iters_time): for data in train_dataloader: traffic, targets = data traffic = traffic.to(device) targets = targets.to(device) outputs = model(traffic) # 将训练的数据放入 # print(traffic.shape,outputs.shape,targets.shape) # print(outputs) # print(targets.float()) loss = loss_fn(outputs, targets.float()) # 得到损失值 optimizer.zero_grad() # 优化过程中首先要使用优化器进行梯度清零 loss.backward() # 调用得到的损失，利用反向传播，得到每一个参数节点的梯度 optimizer.step() # 对参数进行优化 total_train_step += batch_size # 上面就是进行了一次训练，训练次数 +1 # Print some information when the number of iterations divide by batch size equal 200 if total_train_step/batch_size % 200 == 0: print(\"Iterations: {}, time consumptions :{}, Loss: {}\".format(total_train_step, time.time() - iters_time, loss)) iters_time = time.time() # writer.add_scalar(\"train_loss\", loss.item(), total_train_step) def validating(val_dataloader,val_data_size,device,model,loss_fn,time_now): total_val_loss = 0 total_accuracy = 0 # 准确率 with torch.no_grad(): for data in val_dataloader: traffic, targets = data traffic = traffic.to(device) targets = targets.to(device) outputs = model(traffic) loss = loss_fn(outputs, targets.float()) # 这里的 loss 只是一部分数据(data) 在网络模型上的损失 total_val_loss = total_val_loss + loss # 整个测试集的loss accuracy = (outputs.argmax(1) == targets.argmax(1)).sum() # 分类正确个数 total_accuracy += accuracy # 相加 fp = open('log/log.txt','a') fp.write('acc of {} is : '.format(time_now)+str(total_accuracy / val_data_size)+'\\n') fp.close() print(\"The loss in the whole val dataset: {}\".format(total_val_loss)) print(\"The acc of the whole val dataset: {}\".format(total_accuracy / val_data_size)) # print('整体测试集上的混淆矩阵:{}'.format(confusion_matrix())) # writer.add_scalar(\"val_loss\", total_val_loss) # writer.add_scalar(\"val_accuracy\", total_accuracy / val_data_size, total_val_step) def save_checkpoints(time_now,model,model_name,i): name_time_part = str(time_now) name_time_part = name_time_part[5:10]+'|'+name_time_part[11:19] cp_path = Path('checkpoints/model_{}_{}'.format(model_name, name_time_part)) cp_path.mkdir(parents=True, exist_ok=True) torch.save(model, \"checkpoints/model_{}_{}/model_{}.pth\".format(model_name, name_time_part, i)) print(\"model has been saved.\") DanteSU            updated 2022-06-05 10:12:08 "},"DP/":{"url":"DP/","title":"深度学习","keywords":"","body":"深度学习 流量分类 Open Source Related Papers Related Links 姿态迁移 Info 对话推荐 Info 计算机图形学 Info DanteSU            updated 2022-06-05 10:33:26 "},"DP/TrafficClassification/":{"url":"DP/TrafficClassification/","title":"流量分类","keywords":"","body":"Traffic Classification Open Source Related Papers Related Links Lead-in Author = DanteSU ！！！本文禁止在取得作者同意之前以任何形式使用！！！ Brief Intro 随着我国经济的高速发展，人民生活水平日益提高，对高速上网的需求也持续增加。经历了数次通信技术的更新迭代，时至今日，民用 5G 已经逐步在全国范围内铺设开来。伴随着网络技术的快速发展，同一时间段内产生的互联网流量明显增多。根据工信部发布的《2021 年 1~10 月通信业经济运行情况》显示，虽然互联网接入流量增速稍稍放缓，但在未来较长时期增长量仍将基本稳定，随宏观经济波动，疫情持续存在，月累计流量将持续增长。 流量的持续增加也会带来一些日常生活中出现问题的增加，比如互联网服务提供商 (ISP) 及其设备供应商面对的愈发困难的网络管理问题。网络运营商需要及时了解网络中的内容，以便他们能够快速做出反应以支持其各种业务目标。伴随着流量总量的增长和新类型流量的出现，曾经的网络流量分类算法难掩颓势，我们需要更准确、更快速、更易于实现、对新的流量类型也能做出处理的网络流量分类算法来帮助我们解决网络流量管理中的新的问题。 流量分类问题已经研究了二十年，相关研究结果得到了广泛的应用。目前流量分类技术的目的是要检测流量模式，从而优先客户自动分配网络资源，或识别客户使用网络资源是否在某种程度上违反了运营商的服务条款，当然，还有一些调整网络流量解包与调取的顺序以优化用户体验的作用。最近，政府还明确了互联网服务提供商在“合法拦截”违法 IP 数据流量方面的义务。正如电话公司必须支持拦截电话一样，互联网服务供应商越来越受政府要求提供特定个人在特定时间点的网络使用信息的要求。 目前流量分类有基于端口的数据包检测、对有效载荷的检测、对基于统计学的特征检测等方法。这些方法都已经在过去得到很好的应用，但由于通信技术的发展，互联网流量的巨大变化，特别是加密流量的增加，它们的准确性已经在下降。随着机器学习方法的普及，研究人员最近研究了这些方法用于流量分类任务并达到了很高的精度，实现了很好的效果。相比于传统流量分类技术，基于机器学习或者深度学习的技术具有操作简单，资源消耗少，实时性好等优点，可以广泛运用于互联网服务供应商的流量管理系统等地方。与此同时，近几年深度学习技术的快速发展，使得一些算法在很多任务的精度和速度上都达到了前所未有的水平，这使得在流量管理系统中应用一套端到端的方案成为可能。由此我想在基于深度学习技术的网络流量分类这一方面进行一定的研究，并考虑流量分类在一些特殊状态下如面对很多未曾见过的应用流量情况下的表现。 Traditional Technology 基于端口的方法 通过端口号进行流量分类是这项任务最古老和最著名的方法。基于端口的分类器使用数据包的 TCP/UDP 标头中的信息来提取假定与特定应用程序相关联的端口号。提取端口号后，与分配的 IANA TCP/UDP 端口号进行比较，进行流量分类。提取过程简单，端口号不受加密方案的影响。由于提取过程快速，这种方法经常用于防火墙和访问控制列表。众所周知，基于端口的分类是网络流量识别最简单、最快的方法之一。然而，端口混淆、网络地址转换、端口转发、协议嵌入和随机端口分配的普遍性大大降低了这种方法的准确性。根据相关研究，目前只有 30% 到 70% 的互联网流量可以使用基于端口的分类方法进行分类。由于这些原因，需要更复杂的流量分类方法来对现代网络流量进行分类。 有效载荷检测技术 这些技术基于对数据包应用层有效载荷中可用信息的分析。大多数有效负载检测方法，也称为深度数据包检测，使用预定义的模式(如正则表达式)作为每个协议的签名。然后使用派生的模式来区分协议彼此。每当发布新协议时都需要更新模式，并且用户隐私问题是这种方法最重要的缺点之一。虽然有人提出了一种新的深度数据包检测系统，可以在不解密的情况下检查加密的有效负载，从而解决了用户隐私问题，但它只能处理 HTTP 安全流量。 统计和机器学习方法 其中一些方法，主要称为统计方法，有一个有偏见的假设，即每个应用程序的基础流量具有一些几乎独有的统计特征。每种统计方法都使用自己的函数和统计数据。有一种办法是基于数据包到达时间和归一化 阈值的概率密度函数的协议指纹。他们对 HTTP、POP3 和 SMTP 等一组协议的准确率高达 91%。在类似的工作中，还有人考虑了数据包大小的概率密度函数。他们的方案能够识别更广泛的协议，包括文件传输协议 (FTP)、互联网消息访问协议 (IMAP)、SSH 和 TELNET，准确率高达87%。 现如今，已经有了大量机器学习方法来对流量进行分类。比如有贝叶斯神经网络，该网络经过训练可以对包括 Kazaa、BitTorrent、GnuTella 在内的大多数知名 P2P 协议进行分类，并达到 99% 的准确率。使用朴素贝叶斯分类器和核密度估计器可以在同一组应用程序上实现 96% 的准确度。在流量分类问题上，人工神经网络方法可以胜过朴素贝叶斯方法。在“ISCX VPN-nonVPN”流量数据集上发表的两篇最重要的论文都是基于机器学习方法的。还有一些使用与时间相关的特征的研究，例如流的持续时间、每秒流字节数、前向和后向到达间隔时间等，使用 k-最近邻和 C4.5 决策树算法来分类网络流量。最终可以实现了大约 92% 的召回率，使用 C4.5 算法分类了六大类流量，包括网站浏览、电子邮件、聊天、流媒体、文件传输和 IP 语音。还在通过 VPN 隧道传输的同一数据集上使用 C4.5 算法实现了大约 88% 的召回率。还有一些研究手动选择 111 个流动特征并使用 k-最近邻算法在 14 类应用中实现了 94% 的准确率。所有这些方法的主要缺点是特征提取和特征选择阶段基本上是在专家的帮助下完成的。因此，这些方法耗时、昂贵且容易出现人为错误。此外，对于使用 k-最近邻分类器的情况，当用于预测时，该算法的执行时间是一个主要问题。 DeepLearning-based Technology A. 多层感知机 多层感知机(MLP)是第一个神经网络架构，由一个输入层、一个输出层和几个隐藏层神经元组成。每层都有几个神经元，这些神经元与相邻层紧密相连，如图 1 所示。神经元对其输入进行加权求和，并通过非线性激活函数产生输出。从理论上讲，足够密集且足够深的多层感知机可以估计任意函数。然而，由于模型需要学习大量的参数，这个模型通常非常复杂、低效并且难以针对任意复杂问题进行训练。尽管单独使用深度多层感知机已经证明效果很差，但使用几层全连接的神经元的感知机仍然可以作为其他模型中间的组成部分使用。对于网络流量分类，单纯的多层感知机由于其复杂性和准确性低而很少使用。在 [1] 中，将许多深度学习方法与随机森林 (RF) 算法进行了比较，以显示性能差距。他们使用 3 个具有不同标签数量的移动数据集。许多深度学习方法在其中两个数据集中优于随机森林算法。然而，实验设置并不完全公平，因为用于 RF、MLP 和其他深度学习方法的输入特征不同。因此，不应将其结果视为对传统机器学习方法的全面比较。 图 1 多层感知机 B. 卷积神经网络 与多层感知机类似，卷积神经网络 (CNN) 也由几个具有可学习参数的层组成。多层感知机无法很好地处理导致隐藏层中有大量可学习参数的高维数据。卷积神经网络架构，如图 2 所示，通过使用卷积层解决了这个问题。在卷积层中，使用了一组具有少量可学习参数的小内核。同一组内核用于整合输入以产生下一层的输出。通过在层中使用相同的多个内核，可学习参数的数量显着减少。在整个输入上使用这些内核有助于模型更轻松地捕获移位不变特征。 池化层也用于一个或几个卷积层之后进行二次采样。此外，最后的隐藏层通常采用全连接层。[2] 中提出了最简单的卷积神经网络 (CNN) 模型，它基本上用一维向量表示每个流或会话，来输入到卷积神经网络模型。他们的卷积神经网络模型有 2 个卷积层、2 个池化层和 2 个全连接层。它们对每个数据包中的字节进行归一化，并且仅使用前 784 个字节。他们在 12 个类别的加密应用程序数据集上评估他们的模型，并表现出比使用时间序列和统计特征的 C4.5 方法有显着改进。在 [3] 中，作者还使用具有 2 个卷积层、2 个池化层和 3 个全连接层的卷积神经网络来执行协议和应用程序分类任务。他们使用再现内核希尔伯特空间 (RKHS) 嵌入并将早期时间序列数据转换为二维图像。他们的卷积神经网络模型在协议和应用分类任务中优于经典的机器学习方法和多层感知机。 [4] 中使用基于简单一维卷积神经网络的半监督方法对五个谷歌应用程序进行分类。他们训练了一个模型，从具有大量未标记数据集的几个采样数据包中预测整个流的统计特征。然后，他们将权重转移到一个新模型，并重新训练它以执行应用分类任务，只使用几个标记样本。他们展示了使用采样时间序列特征而不是前 n 个数据包的可能性，这对于高带宽操作网络可行性更高 [4]。 图 2 卷积神经网络 C. 循环神经网络 循环神经网络 (RNN) 是包含用于存储时间信息的循环的神经网络，如图3 所示。循环神经网络是专门为顺序数据设计的，其中输出可能不仅取决于最后一个输入，还取决于之前的输入。循环神经网络已成功应用于语音识别、时间序列预测、翻译和语言建模等一系列任务。梯度消失和爆炸使得学习长期的依赖关系变得困难(例如，相距很远的输入之间的依赖)，是传统循环神经网络中的一个常见障碍。长短期记忆 (LSTM) 通过添加一组控制何时存储或删除 信息的门来缓解这些问题。对于网络分类任务，研究发现混合模型优于纯长短期记忆模型或卷积神经网络模型 [5]。卷积神经网络和循环神经网络都在 [6]、[5] 中用于不同的应用来捕捉流的空间和时间特征。除了微小的差异之外，两项研究都将前 6 到 30 个数据包的内容带到卷积神经网络模型、循环神经网络或长短期记忆模型。尽管确切的输入特征、神经网络架构和数据集不同，但它们都达到了很高的精度。尽管它们在序列数据方面取得了成功，但长短期记忆并不适合需要显式和外部存储器的复杂任务。最近引入了新的架构，例如记忆网络和神经图灵机 (NTM)，以将显式记忆嵌入到架构中，称为记忆增强神经网络 (MANN)。记忆增强神经网络已成功应用于语言建模、问答和一次性学习。但尚未研究记忆增强神经网络在网络分类任务上的性能。 图 3 循环神经网络 D. 自编码器 (AE) 相对于上述模型，自编码器 (AE) 具有明显更小的隐藏层的神经网络，目的是在输出端重建输入，如图 4 所示。内部的编码表示可用于数据压缩或降维。多层感知机、卷积神经网络和循环神经网络都可以用作自编码器架构的一部分。自编码器广泛用于初始化深度架构的权重。自编码器有一些变体，例如去噪自编码器 (DAE) 通过输入带噪音的样本来输出完整的输入样本，从而迫使模型学习更稳健的特征，以及用于生成的变分自编码器 (VAE)可以产生模拟目标分布的虚假数据。更复杂的架构，称为堆叠式自动编码器 (SAE)，堆叠了多个自编码器，其中每个自编码器的输出都是下一个自编码器的输入，整个模型以贪婪的逐层方式进行训练。也可以训练一个混合学习框架，它将自编码器与多层感知机或其他模型相结合，从一开始就带有标记数据。因此，该模型同时学习输入和输出分布。这种混合模型使用多目标损失函数进行训练，包括标准输出损失以及重建损失。自编码器通常使用无监督的方式来获得输入数据的表示，这些表示以后可以用作分类器的一部分。例如，在 [7] 中，使用自编码器来重构输入。然后，将 softmax 层应用于自编码器的编码内部表示，最后达到了不错的精度。他们使用自己的私有数据集，包含 7 种流量类型。此外，他们使用 12 个区间和两个流向的 9 个统计特征作为输入。在 [8] 中，作者使用标头和有效负载数据在 ISCX VPN non-VPN 数据集上训练一维卷积神经网络和堆叠式自编码器模型。两种模型都显示出很高的准确性，但卷积神经网络模型的性能略优于堆叠式自编码器模型。 图 4 自编码器 E. 生成对抗网络 生成对抗网络 (GAN) 是一种无监督技术，可同时训练生成模型和判别模型。如图 5 所示，生成器旨在生成目标分布的数据，而判别器模型旨在区分真实数据和生成数据。这两种模型通常都是神经网络。首先训练生成器以通过判别器最大化错误概率。然后，固定生成器并训练鉴别器以最小化错误概率，同时输入真实和生成的数据。继续该过程直到收敛。尽管生成对抗网络难以训练和收敛，但它已被用于许多应用中，例如创建逼真的图像、从图像重建 3D 模型、提高图像质量、为数据稀缺的应用创建合成数据。生成模型可用于处理网络流量分类中的数据集不平衡问题。不平衡问题是指每个类别的样本数量变化很大的情况。在这种情况下，机器学习算法通常难以正确预测数据较少的类别。处理不平衡数据集最常见和最简单的方法是对数据较少的类别进行过采样，复制产生更多的样本，或对数据较多的类别进行欠采样，从其中删除一些样本。在 [9] 中，辅助分类的生成对抗网络(AC- GAN)可以生成用于监督网络分类任务的合成样本。 AC-GAN 和传统生成对抗网络的主要区别在于 AC-GAN 同时将随机噪声和类标签作为输入，从而生成输入类标签的样本。他们使用具有两个类(SSH 和非 SSH)的公共数据集、 22 个统计特征来作为输入。他们仅使用深度模型来生成合成数据。对于分类部分，他们使用经典的机器学习算法，包括支持向量机、随机森林和决策树。 图 5 生成对抗网络 F. 图神经网络 图神经网络(GNN)是一种直接作用于图结构上的神经网络。如图 6 所示，图神经网络的一个典型应用是节点分类，本质上，图中的每个节点都与一个标签相关联，我们希望预测未标记节点的标签。在节点分类问题中，每个节点都可以用其特征表示并且与已标记的标签相关联。给定部分标记的图，目标是利用这些标记的节点来预测未标记的节点标签。它通过学习得到每个节点的高维向量(状态)表示，同时包含其相邻节点的信息。Shen 等人 [10] 提出了一种称为“流量交互图(TIG)”的图结构，基于图神经网络分类器，识别去中心化应用程序(DApp)。该文采用图来表征流量交互特征，将流量按流划分，每条流包含一系列数据包，每个包以五元组(源/目的 IP，源/目的端口，协议)表示。从用户角度看，将上行流数据包长度设为负数，将下行流数据包长度设为正数。 图 6 图神经网络 System Design 对于网络流量数据包的分类，我们需要训练出一个基于机器学习的模型来处理此问题。因为现阶段，深度学习发展如火如荼，有很多比较新的办法在很多问题上都有很好的效果，所以我们选取深度学习方法来解决此问题。对于深度学习中的模型训练而言，主要有两个最重要的关键，一个是模型的选择，另一个是数据的选取。流程设计如图 7 所示。首先我们将选取的数据集进行预处理来选择合适的特征，之后通过特征降维得到可以用来训练的样本数据，输入进入机器学习模型训练，经过一系列参数调整与训练，我们可以得到最终的一个分类器。将待分类样本输入进入训练完成的分类模型就可以得到分类的结果了。 图 7 流程图 Datasets 对于网络流量数据，以相关论文采用的开源数据集(比如 ISCX-VPN2016[11], ISCX2012[12], UNSW-NB15[13][14][15][16][17])为基础，根据直接搜集法与脚本生成法，以及其他类似数据集的收集办法，以多个数据集结合，交叉验证的办法(混合生成法)。 直接搜集法: 借助 Wireshark、Tcpdump、Sniffer 等抓包工具在真实环境中捕捉数据制作数据集。 脚本生成法:模拟攻击流量常利用脚本或虚拟网络产生，如模拟 APT 攻击、 IOT 攻击、内网渗透等攻击流量来组成数据集。 DanteSU            updated 2022-06-05 10:33:05 "},"DP/TrafficClassification/1.html":{"url":"DP/TrafficClassification/1.html","title":"Open Source","keywords":"","body":"Open source Useful Models Author = DanteSU Headers import torch from torch import nn, Tensor import torch.nn.functional as F from torch.nn import TransformerEncoder, TransformerEncoderLayer Deep Packet CNN # model 1 class DeepPacketCNN(nn.Module): def __init__(self, n_classes, batch_size): super(DeepPacketCNN, self).__init__() self.batch_size = batch_size self.conv1 = nn.Conv1d(1, 200, 5, 2, 0) self.bn1 = nn.BatchNorm1d(200) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv1d(200, 100, 4, 1, 0) self.bn2 = nn.BatchNorm1d(100) self.pool = nn.AvgPool1d(2) self.dropout = nn.Dropout(p=0.25) self.fc1 = nn.Sequential(nn.Linear(100 * 372, 600), # 00 * 372 nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc2 = nn.Sequential(nn.Linear(600, 500), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc3 = nn.Sequential(nn.Linear(500, 400), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc4 = nn.Sequential(nn.Linear(400, 300), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc5 = nn.Sequential(nn.Linear(300, 200), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc6 = nn.Sequential(nn.Linear(200, 100), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc7 = nn.Sequential(nn.Linear(100, 50), nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc_out = nn.Linear(50, n_classes) self.lsm = nn.LogSoftmax(dim=0) def forward(self, x): # print('x1x1x1x1x1x1x1x1', x.shape) x = self.conv1(x) # print('x2x2x2x2x2x2x2x2', x.shape) x = self.bn1(x) # print('x3x3x3x3x3x3x3x3', x.shape) x = self.relu(x) # print('x4x4x4x4x4x4x4x4', x.shape) x = self.conv2(x) # print('x5x5x5x5x5x5x5x5', x.shape) x = self.bn2(x) # print('x6x6x6x6x6x6x6x6', x.shape) x = self.relu(x) # print('x7x7x7x7x7x7x7x7', x.shape) x = self.pool(x).view(self.batch_size,-1) # print('x8x8x8x8x8x8x8x8', x.shape) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) x = self.fc4(x) x = self.fc5(x) x = self.fc6(x) x = self.fc7(x) x = self.fc_out(x) # print('xxxxxxxxxxxxxxxxxx',x.shape) y = self.lsm(x) # print('yyyyyyyyyyyyyyyyyy',y.shape) return y SAE # model 2 class AutoEncoder(nn.Module): def __init__(self, input_size, output_size): super(AutoEncoder, self).__init__() self.forward_pass = nn.Sequential(nn.Linear(input_size, output_size), nn.Dropout(p=0.05), nn.ReLU(True) ) self.backward_pass = nn.Linear(output_size, input_size) self.criterion = nn.MSELoss() self.optimizer = torch.optim.SGD(self.parameters(), lr=0.1) def forward(self, x): x = x.detach() y = self.forward_pass(x) if self.training: x_reconstruct = self.backward_pass(y) loss = self.criterion(x_reconstruct, x.data) self.optimizer.zero_grad() loss.backward() self.optimizer.step() return y.detach() class StackedAutoEncoder(nn.Module): def __init__(self, n_classes): super(StackedAutoEncoder, self).__init__() self.ae1 = AutoEncoder(1500, 400) self.ae2 = AutoEncoder(400, 300) self.ae3 = AutoEncoder(300, 200) self.ae4 = AutoEncoder(200, 100) self.ae5 = AutoEncoder(100, 50) self.fc_out = nn.Linear(50, n_classes) self.lsm = nn.LogSoftmax(dim=0) def forward(self, x): x = self.ae1(x) x = self.ae2(x) x = self.ae3(x) x = self.ae4(x) x = self.ae5(x) y = self.fc_out(x) y = self.lsm(y) return y # model 3 class trafficBert(nn.Module): def __init__(self,n_classes): super(trafficBert, self).__init__() self.bert = BertModel.from_pretrained('bert_pretrain') # /bert_pretrain/ for param in self.bert.parameters(): param.requires_grad = True # 每个参数都要 求梯度 self.fc = nn.Linear(768, n_classes) # 768 -> 2 def forward(self, x): print('_'*20, x.shape) context = x[0] # 输入的句子 (ids, seq_len, mask) types = x[1] mask = x[2] # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0] _, pooled = self.bert(context, token_type_ids=types, attention_mask=mask, output_all_encoded_layers=False) # 控制是否输出所有encoder层的结果 out = self.fc(pooled) # 得到10分类 return out ResNet # model 4 class ResNet(nn.Module): def __init__(self, ch_in, ch_out): super(ResNet, self).__init__() self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(ch_out) self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(ch_out) self.relu = nn.ReLU(inplace=True) self.extra = nn.Sequential() # 如果输入、输出维度不同，需转化后才能相加 if ch_out != ch_in: self.extra = nn.Sequential( nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1), nn.BatchNorm2d(ch_out) ) def forward(self, x): # print('-1'*10,x.shape) out = self.conv1(x) # print('-2'*10,out.shape) out = self.bn1(out) # print('-3'*10,out.shape) out = self.relu(out) # print('-4'*10,out.shape) out = self.conv2(out) # print('-5'*10,out.shape) out = self.bn2(out) # print('-6'*10,out.shape) out = self.extra(x) + out # print('-7'*10,out.shape) return out class ResNet18(nn.Module): def __init__(self,n_classes): super(ResNet18, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64) ) self.relu = nn.ReLU(inplace=True) # 4 block [b, 64, h, w] => [b, 1024, h, w] self.blk1 = ResNet(64, 128) self.blk2 = ResNet(128, 256) self.blk3 = ResNet(256, 512) self.blk4 = ResNet(512, 1024) # 注意最后全连接层维度，进去之前需要先打平 self.outlayer = nn.Linear(1024*30*50, n_classes) def forward(self, x): x = x.view(len(x),30,-1) x = x.unsqueeze(1) # print('-1'*10,x.shape) x = self.conv1(x) # print('-2'*10,x.shape) x = self.relu(x) # print('-3'*10,x.shape) # x = self.blk1(x) # print('-4'*10,x.shape) x = self.blk2(x) # print('-5'*10,x.shape) x = self.blk3(x) # print('-6'*10,x.shape) x = self.blk4(x) # print('-7'*10,x.shape) # print(x.size(0)) x = x.view(x.size(0), -1) # 先打平，再进全连接 # print('-8'*10,x.shape) x = self.outlayer(x) # print('-9'*10,x.shape) return x LSTM Pure LSTM # model 5 class LSTM(nn.Module): def __init__(self,INPUT_SIZE,n_classes): super(LSTM, self).__init__() self.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns input_size=INPUT_SIZE, hidden_size=64, # rnn hidden unit num_layers=1, # number of rnn layer batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size) ) self.out = nn.Linear(64, n_classes) def forward(self, x): # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state # choose r_out at the last time step out = self.out(r_out[:, -1, :]) return out LSTM+FC # model 5.1 class LSTM_FC2(nn.Module): def __init__(self,INPUT_SIZE,n_classes): super(LSTM_FC2, self).__init__() self.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns input_size=INPUT_SIZE, hidden_size=1024, # rnn hidden unit num_layers=1, # number of rnn layer batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size) ) self.fc1 = nn.Sequential(nn.Linear(1024,256), # 00 * 372 nn.Dropout(p=0.25), nn.ReLU(True) ) self.fc2 = nn.Sequential(nn.Linear(256, 64), nn.Dropout(p=0.25), nn.ReLU(True) ) self.out = nn.Linear(64, n_classes) self.lsm = nn.LogSoftmax(dim=0) def forward(self, x): # print(x.shape) # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state # print('r_out is : ',r_out.shape) r_out = r_out[:, -1, :] # print('r_out is : ',r_out.shape) r_out = self.fc1(r_out) r_out = self.fc2(r_out) # choose r_out at the last time step out = self.out(r_out) # print('out is : ',out.shape) return out CNN+Transformer # model 6 class ResModule(nn.Module): def __init__(self, ch_in, ch_out, kernel_size, stride): super(ResModule, self).__init__() self.conv1 = nn.Conv1d(ch_in, ch_out, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2) self.bn1 = nn.BatchNorm1d(ch_out) self.conv2 = nn.Conv1d(ch_out, ch_out, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2) self.bn2 = nn.BatchNorm1d(ch_out) self.relu = nn.ReLU(inplace=True) if stride == 1: self.downsample = None else: self.downsample = nn.Sequential( nn.Conv1d(ch_in, ch_out, kernel_size=1, stride=stride), nn.BatchNorm1d(ch_out)) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: x = self.downsample(x) return self.relu(x+out) class CNN(nn.Module): def __init__(self,ch): super(CNN, self).__init__() self.conv1 = nn.Sequential( nn.Conv1d(1, ch, kernel_size=3, stride=1, padding=1), nn.BatchNorm1d(ch) ) self.relu = nn.ReLU(inplace=True) self.blk1 = ResModule(ch, ch*2, 3, 2) self.blk2 = ResModule(ch*2, ch*2, 3, 2) self.blk3 = ResModule(ch*2, ch*4, 3, 2) self.blk4 = ResModule(ch*4, ch*4, 3, 2) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = self.blk1(x) x = self.blk2(x) x = self.blk3(x) x = self.blk4(x) return x class PositionalEncoding(nn.Module): def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000): super().__init__() self.dropout = nn.Dropout(p=dropout) position = torch.arange(max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) pe = torch.zeros(max_len, 1, d_model) pe[:, 0, 0::2] = torch.sin(position * div_term) pe[:, 0, 1::2] = torch.cos(position * div_term) self.register_buffer('pe', pe) def forward(self, x: Tensor) -> Tensor: \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" x = x + self.pe[:x.size(0)] return self.dropout(x) class CNN_TRAN(nn.Module): def __init__(self, n_classes): super(CNN_TRAN, self).__init__() self.cnn = CNN(ch=16) emsize = 64 self.pos_encoder = PositionalEncoding(emsize, dropout=0.2) encoder_layers = TransformerEncoderLayer(d_model=emsize, nhead=4, dim_feedforward=256, dropout=0.2) self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=4) self.d_model = emsize self.decoder = nn.Sequential( nn.Linear(64*94, 512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, n_classes) ) # self.init_weights() def init_weights(self) -> None: pass def forward(self, src: Tensor) -> Tensor: bs,_,_ = src.shape x = self.cnn(src).permute(2,0,1) x = self.pos_encoder(x) x = self.transformer_encoder(x).permute(1,2,0) x = self.decoder(x.reshape(bs,-1)) pred = F.softmax(x,dim=1) return pred LSTNet # model 7 class LSTNet(nn.Module): def __init__(self, window=50, input_dim=1500,hidRNN=100,hidCNN=250,hidSkip=5,CNN_kernel=5,skip=5, highway_window=1500,dropout=float(0.2),output_fun='sigmoid'): super(LSTNet, self).__init__() # P1500 m 1 self.use_cuda = True self.P = window# 时间序列窗口，输入网络的时间戳长度 self.m = 30; # 时间序列输入维度 self.hidR = hidRNN # RNN层输出维度 self.hidC = hidCNN # CNN层输出维度 self.hidS = hidSkip # Skip-RNN层输出维度 self.Ck = CNN_kernel # CNN层kernel大小 self.skip = skip # 周期长度 self.pt = int((self.P - self.Ck)/self.skip) # window在kernel作用下，以skip为周期的数据数量。周期数目。 #50-5/5 =9 self.hw = highway_window # highway通道的输出节点数目 self.conv1 = nn.Conv2d(1, self.hidC, kernel_size = (self.Ck, self.m)) # 第一层conv2d self.GRU1 = nn.GRU(self.hidC, self.hidR) # 第一层GRU self.dropout = nn.Dropout(p = dropout) if (self.skip > 0): self.GRUskip = nn.GRU(self.hidC, self.hidS) # self.linear1 = nn.Linear(self.hidR + self.skip * self.hidS, self.m) self.linear1 = nn.Linear(self.hidR + self.skip * self.hidS, 11) else: self.linear1 = nn.Linear(self.hidR, self.m) if (self.hw > 0): # self.highway = nn.Linear(self.hw, 1) # highway后跟线性层 self.highway = nn.Linear(self.hw, 11) self.output = None if (output_fun == 'sigmoid'): self.output = F.sigmoid if (output_fun == 'tanh'): self.output = F.tanh def forward(self, x): # print('x here is : ',x.shape)#[2, 1, 1500] batch_size = x.size(0) # print('batch_size here is : ',batch_size)#2 # 输入 x dim = [batch_size, window_size, data_dim] # 因为 CNN conv2d need [batch_size, in_channels, inputHeight, inputWidth], # 所以 x 转变为 c = [batch_size, 1, window_size, data_dim] c = x.view(-1, 1, self.P, self.m)#[4, 1, 1500, 1] # print('c1 here is : ',c.shape) # 经过 conv1 后，out = [batch_size, out_channels, outHeight, outWidth], # 其中 outWidth = self.m 即时间序列输入维度，即每次kernel滑动都会将输入维度变为1, # 所以 outWidth=1，需要 squeeze 为3个维度 [batch_size, out_channels, outHeight] c = F.relu(self.conv1(c))#2, 250, 46, 1] # print('c2 here is : ',c.shape) c = self.dropout(c) # print('c3 here is : ',c.shape) c = torch.squeeze(c, 3); # batch_size, out_channels, outHeight] # print('c4 here is : ',c.shape) # RNN # GRU input shape: [seq_len, batch_size, input_size]，需要由 c 先 reshape r = c.permute(2, 0, 1).contiguous() # print('r here is : ',r.shape) _, r = self.GRU1(r) r = self.dropout(torch.squeeze(r,0)) # [batch_size, hiddenRnn] # print('r here is : ',r.shape) #skip-rnn if (self.skip > 0): # print('here is skip!') s = c[:,:, int(-self.pt * self.skip):].contiguous(); # print('s1 here is : ',s.shape)#5 100 45 # print('-----------',batch_size, self.hidC, self.pt, self.skip) s = s.view(batch_size, self.hidC, int(self.pt), self.skip) #2,250,9,5 # print('s2 here is : ',s.shape) s = s.permute(2,0,3,1).contiguous() # print('s3 here is : ',s.shape) s = s.view(self.pt, batch_size * self.skip, self.hidC) # print('s4 here is : ',s.shape) _, s = self.GRUskip(s) s = s.view(batch_size, self.skip * self.hidS) # print('s5 here is : ',s.shape) s = self.dropout(s); # [batch_size, hiddenSkip] # print('s6 here is : ',s.shape) # 把(batch_size, hiddenRnn)和(batch_size, hiddenSkip)拼接在一起， # 然后通过全连接，得到(batch_size, data_dim)的非线性输出 r = torch.cat((r,s),1) # print('r here is : ',r.shape) res = self.linear1(r) # [batch_size, data_dim(即self.m)] 这个的输出和输入是一样大的，不对 # print('res here is : ',res.shape) #highway if (self.hw > 0): # print('here is hw') z = x[:, -self.hw:, :] # print('z here is : ',z.shape) z = z.permute(0,2,1).contiguous().view(-1, self.hw) # print('z here is : ',z.shape)#4,1500 z = self.highway(z) # 过线性层 # print('z here is : ',z.shape)#2,1 # z = z.view(-1,self.m)#30 # print('z here is : ',z.shape) res = res + z# 相加 2,12 # print('res here is : ',res.shape) if (self.output): res = self.output(res)# 过一层激活函数，论文里表示relu效果比tanh更好 return res Data Processing from pcap to pickle import os import time from scapy.all import * #from pcapng.scanner import FileScanner import numpy as np import os import multiprocessing as mp import pickle as pk def pkts2X(pkts): X = [] #lens = [] for p in pkts: #=================================== # step 1 : remove Ether Header #=================================== r = raw(p)[14:] r = np.frombuffer(r, dtype = np.uint8) #p.show() #=================================== # step 2 : pad 0 to UDP Header # it seems that we need to do nothing this step # I found some length of raw data is larger than 1500 # remove them. #=================================== if (TCP in p or UDP in p): \"\"\" if UDP in p: # todo : padding 0 to print ('UDP', r[:20]) print(p[IP].src, p[IP].dst) else : print('TCP', r[:20]) print(p[IP].src, p[IP].dst) \"\"\" if (len(r) > 1500): pass else: X.append(r) #lens.append(len(r)) else: pass return X#, lens def get_data_by_file(filename): pkts = rdpcap(filename) X = pkts2X(pkts) # save X to npy and delete the original pcap (it's too large). return X def task(filename): global dict_name2label global counter head, tail = os.path.split(filename) cond1 = os.path.isfile(os.path.join('data', tail+'.pickle')) cond2 = os.path.isfile(os.path.join('data', tail+'_class.pickle')) if (cond1 and cond2): with lock: counter.value += 1 print('[{}] {}'.format(counter, filename)) return '#ALREADY#' X = get_data_by_file(filename) if (not cond1): y = [dict_name2label[tail]] * len(X) with open(os.path.join('data', tail+'.pickle'), 'wb') as f: pk.dump((X, y), f) if (not cond2): y2 = [dict_name2class[tail]] * len(X) with open(os.path.join('data', tail+'_class.pickle'), 'wb') as f: pk.dump(y2, f) with lock: counter.value += 1 print('[{}] {}'.format(counter, filename)) return 'Done' def gen_todo_list(directory, check = None): files = os.listdir(directory) todo_list = [] for f in files: fullpath = os.path.join(directory, f) if os.path.isfile(fullpath): if check is not None: if check(f): todo_list.append(fullpath) else: todo_list.append(fullpath) return todo_list #========================================= # mp init #========================================= with open('fileName2Application.pickle', 'rb') as f: dict_name2label = pk.load(f) with open('fileName2Characterization.pickle', 'rb') as f: dict_name2class = pk.load(f) lock = mp.Lock() counter = mp.Value('i', 0) cpus = mp.cpu_count()//2 pool = mp.Pool(processes=cpus) todo_list = gen_todo_list('../pcap') #todo_list = todo_list[:3] total_number = len(todo_list) done_list = [] res = pool.map(task, todo_list) print(len(res)) Links to two files needed fileName2Application.pickle fileName2Characterization.pickle from pickle to npy/txt import time # 准备数据集 import torch from torch.utils.data import Dataset, DataLoader import pickle as pk from sklearn.preprocessing import LabelBinarizer import numpy as np import time import os class MyDataset(Dataset): \"\"\" 下载数据、初始化数据，都可以在这里完成 \"\"\" def __init__(self,data_path): self.data_path = data_path self.data = self.data(task_type = 'class') self.X_train = torch.from_numpy(np.array(self.data[0])).to(torch.float32) self.X_val = torch.from_numpy(np.array(self.data[1])).to(torch.float32) self.X_test = torch.from_numpy(np.array(self.data[2])).to(torch.float32) self.y_train_onehot = torch.from_numpy(np.array(self.data[3])) self.y_val_onehot = torch.from_numpy(np.array(self.data[4])) self.y_test_onehot = torch.from_numpy(np.array(self.data[5])) def data(self, task_type): # load 資料 data_time = time.time() X_train, y_train, X_val, y_val, X_test, y_test = self.load_data(task_type) print('The time consumption of loading data is : {}'.format(time.time()-data_time)) # print('yyyyyyyyyyyyyyyy', y_train[1]) # normalize X # X_train , X_val, X_test = np.array(X_train) / 255, np.array(X_val) / 255, np.array(X_test) / 255 # 把 y 的 string 做成 one hot encoding 形式 coding_time = time.time() label_encoder = LabelBinarizer() y_train_onehot = label_encoder.fit_transform(y_train) y_val_onehot = label_encoder.transform(y_val) y_test_onehot = label_encoder.transform(y_test) print('The time consumption of coding in one-hot is : {}'.format(time.time()-coding_time)) data = [X_train , X_val, X_test, y_train_onehot, y_val_onehot, y_test_onehot] return data def dump(self, data, filename): with open(filename, 'wb') as f: pk.dump(data, f) def load(self, filename): with open(filename, 'rb') as f: data = pk.load(f) return data def gen_todo_list(self, check = None): files = os.listdir(self.data_path) todo_list = [] for f in files: fullpath = os.path.join(self.data_path, f) if os.path.isfile(fullpath): if check is not None: if check(f): todo_list.append(fullpath) else: todo_list.append(fullpath) return todo_list def check(self, filename): return not '_class' in filename def load_data(self, task_type): max_data_nb = 10000 todo_list = self.gen_todo_list(check = self.check) train_rate = 0.64 val_rate = 0.16 X_train = [] y_train = [] X_val = [] y_val = [] X_test = [] y_test = [] for counter, filename in enumerate(todo_list): (tmpX, tmpy) = self.load(filename) if task_type == 'class': tmpy = self.load('.'.join(filename.split('.')[:-1]) + '_class.pickle') tmpX , tmpy = tmpX[:max_data_nb], tmpy[:max_data_nb] assert(len(tmpX) == len(tmpy)) tmpX = self.processX(tmpX) train_num = int(len(tmpX) * train_rate) val_num = int(len(tmpX) * val_rate) X_train.extend(tmpX[:train_num]) y_train.extend(tmpy[:train_num]) X_val.extend(tmpX[train_num: train_num + val_num]) y_val.extend(tmpy[train_num: train_num + val_num]) X_test.extend(tmpX[train_num + val_num:]) y_test.extend(tmpy[train_num + val_num:]) print('\\rLoading... {}/{}'.format(counter+1,len(todo_list)), end = '') print('\\r{} Data loaded. '.format(len(todo_list))) return X_train, y_train, X_val, y_val, X_test, y_test def processX(self, X): X = np.array(X) lens = [len(x) for x in X] maxlen = 1500 tmpX = np.zeros((len(X), maxlen)) mask = np.arange(maxlen) = len(self.index_list): raise StopIteration return self.index_list[self.pos] def __iter__(self): self.pos = -1 return self def __len__(self): return len(self.index_list) def do(data_path = '../data'): print('preparing dataset...') trainset = MyDataset(data_path=data_path) # 获得数据集的长度 len(), 即length train_data_size = len(trainset.X_train) # 格式化字符串, format() 中的数据会替换 {} print(\"训练数据集的长度为: {}\".format(train_data_size)) # 在GPU上加载数据 print('loading X_train...') X_train = trainset.X_train print(len(X_train)) print('loading y_train_onehot...') y_train_onehot = trainset.y_train_onehot print(len(y_train_onehot)) print('loading X_val...') X_val = trainset.X_val print(len(X_val)) print('loading y_val_onehot...') y_val_onehot = trainset.y_val_onehot print(len(y_val_onehot)) print('loading X_test...') X_test = trainset.X_test print(len(X_test)) print('loading y_test_onehot...') y_test_onehot = trainset.y_test_onehot print(len(y_test_onehot)) print('loading is done.') print('saving x_train...') np.save('../data2/x_train', X_train) print('saving x_val...') np.save('../data2/x_val', X_val) print('saving x_test...') np.save('../data2/x_test', X_test) print('saving y_train...') np.save('../data2/y_train_onehot', y_train_onehot) print('saving y_val...') np.save('../data2/y_val_onehot', y_val_onehot) print('saving y_test...') np.save('../data2/y_test_onehot', y_test_onehot) def do2(): print('saving x_train...') X_train = np.load('../data2/x_train.npy') np.savetxt('../data2/x_train.txt', X_train) print('saving x_val...') X_val = np.load('../data2/x_val.npy') np.savetxt('../data2/x_val.txt', X_val) print('saving x_test...') X_test = np.load('../data2/x_test.npy') np.savetxt('../data2/x_test.txt', X_test) print('saving y_train...') y_train_onehot = np.load('../data2/y_train_onehot.npy') np.savetxt('../data2/y_train_onehot.txt', y_train_onehot) print('saving y_val...') y_val_onehot = np.load('../data2/y_val_onehot.npy') np.savetxt('../data2/y_val_onehot.txt', y_val_onehot) print('saving y_test...') y_test_onehot = np.load('../data2/y_test_onehot.npy') np.savetxt('../data2/y_test_onehot.txt', y_test_onehot) if __name__ == '__main__': do2() DanteSU            updated 2022-06-05 10:55:27 "},"DP/TrafficClassification/2.html":{"url":"DP/TrafficClassification/2.html","title":"Related Papers","keywords":"","body":"Related Papers Author = DanteSU English paper [1] Aceto G, D Ciuonzo, Montieri A, et al. Mobile Encrypted Traffic Classification Using Deep Learning[C]// IEEE/ACM Network Traffic Measurement and Analysis Conference (TMA'18). ACM, 2018. [2] Wei W, Ming Z, Wang J, et al. End-to-end encrypted traffic classification with one-dimensional convolution neural networks[C]// 2017 IEEE International Conference on Intelligence and Security Informatics (ISI). IEEE, 2017. [3] Chen Z, Ke H, Jian L, et al. Seq2Img: A sequence-to-image based approach towards IP traffic classification using convolutional neural networks[C]// 2017 IEEE International Conference on Big Data (Big Data). IEEE, 2017. [4] Rezaei S, Liu X. how to achieve high classification accuracy with just a few labels: a semi-supervised approach using sampled packets *[J]. 2019. [5] Lopez-Martin M, Carro B, Sanchez-Esguevillas A, et al. Network Traffic Classifier With Convolutional and Recurrent Neural Networks for Internet of Things[J]. IEEE Access, 2017, PP(99):1-1. [6] Wei W, Sheng Y, Wang J, et al. HAST-IDS: Learning Hierarchical Spatial- Temporal Features Using Deep Neural Networks to Improve Intrusion Detection[J]. IEEE Access, 2018, 6(99):1792-1806. [7] Hchst J, Baumgrtner L, Hollick M, et al. Unsupervised Traffic Flow Classification Using a Neural Autoencoder[C]// 2017 IEEE 42nd Conference on Local Computer Networks (LCN). IEEE, 2017. [8] LotfollahiMohammad, Siavoshanimahdi J, Zaderamin S H, et al. Deep packet: a novel approach for encrypted traffic classification using deep learning[J]. Soft Computing, 2019. [9] Vu L, Bui C T, Nguyen Q U. A Deep Learning Based Method for Handling Imbalanced Problem in Network Traffic Classification[C]// Eighth International Symposium on Information & Communication Technology. ACM, 2017:333-339. [10] Shen M, Zhang J, Zhu L, et al. Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks[J]. IEEE Transactions on Information Forensics and Security, 2021, PP(99):1-1. [11] Lashkari A H, Draper-Gil G, Mamun M, et al. Characterization of Encrypted and VPN Traffic Using Time-Related Features[C]// The International Conference on Information Systems Security and Privacy (ICISSP). 2016. [12] Shiravi A, Shiravi H, Tavallaee M, et al. Toward developing a systematic approach to generate benchmark datasets for intrusion detection[J]. Computers & Security, 2012, 31(3):357-374. [13] Moustafa N, Slay J. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)[C]// Military Communications and Information Systems Conference (MilCIS), 2015. IEEE, 2015. [14] Moustafa N, Slay J. The evaluation of Network Anomaly Detection Systems: Statistical analysis of the UNSW-NB15 data set and the comparison with the KDD99 data set[J]. Information Security Journal A Global Perspective, 2016:1-14. [15] Moustafa N, et al. Novel Geometric Area Analysis Technique for Anomaly Detection Using Trapezoidal Area Estimation on Large-Scale Networks[J]. IEEE Transactions on Big Data, 2017. [16] Moustafa N, Creech G, Slay J. Big Data Analytics for Intrusion Detection System: Statistical Decision-Making Using Finite Dirichlet Mixture Models[J]. 2017. [17] Sarhan M, Layeghy S, Moustafa N, et al. NetFlow Datasets for Machine Learning-based Network Intrusion Detection Systems[J]. 2020. [18] Stber T, Frank M, Schmitt J, et al. Who do you sync you are?: smartphone fingerprinting via application behaviour[M]. 2013. [19] Aceto G, Ciuonzo D, Montieri A, et al. Traffic Classification of Mobile Apps through Multi-Classification[C]// IEEE Global Communications Conference (Globecom). IEEE, 2017. [20] Ran D, Dvir A, Pele O, et al. I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video Streaming Title Classification[J]. IEEE Transactions on Information Forensics and Security, 2017, PP(12):3039-3049. [21] Taylor V F, Spolaor R, Conti M, et al. Robust Smartphone App Identification Via Encrypted Network Traffic Analysis[J]. IEEE Transactions on Information Forensics and Security, 2017, 13(1):63-78. [22] J. Zhang, et al. Robust network traffic classification, IEEE/ACM Transactions on Networking (TON), vol. 23, no. 4, 2015, pp. 1257-1270. [23] Woodward M, Finn C. Active One-shot Learning[J]. NIPS (2016) Deep Reinforcement Learning Workshop, 2018. [24] Rezaei S, Liu X. Deep Learning for Encrypted Traffic Classification: An Overview[J]. IEEE Communications Magazine, 2019, 57(5):76-81. Chinese Papers [25] 冷涛.基于深度学习的加密流量分类研究综述[J].计算机与现代化,2021(08):112-120. [26] 张稣荣,卜佑军,陈博,孙重鑫,王涵,胡先君.基于多层双向 SRU 与注意力模型的加密流量分类方法 [J/OL]. 计算机工程 :1-15[2022-03- 22].DOI:10.19678/j.issn.1000-3428.0063626. [27] 皇甫雨婷,李丽颖,王海洲,沈富可,魏同权.自注意力的多特征网络流量异常检测与分类[J].华东师范大学学报(自然科学版),2021(06):161-173. DanteSU            updated 2022-06-05 23:19:17 "},"DP/TrafficClassification/3.html":{"url":"DP/TrafficClassification/3.html","title":"Related Links","keywords":"","body":"Related Links Author = DanteSU Datasets 名称 网址 CICDataset http://205.174.165.80/ VPN 2016 - Datasets - Research - Canadian Institute for Cybersecurity - UNB https://www.unb.ca/cic/datasets/vpn.html 网络安全数据集汇总 AI Paddle https://aistudio.baidu.com/aistudio/projectdetail/1236018 网络安全数据集 https://blog.csdn.net/Jesusons/article/details/117459736 IP Network Traffic Flows Labeled with 75 Apps https://www.kaggle.com/datasets/jsrojas/ip-network-traffic-flows-labeled-with-87-apps/code ISCXTor2016 https://blog.csdn.net/wisemanchen/article/details/111877592 Code 名称 网址 Deep Packet https://blog.munhou.com/2020/04/05/Pytorch-Implementation-of-Deep-Packet-A-Novel-Approach-For-Encrypted-Tra%EF%AC%83c-Classi%EF%AC%81cation-Using-Deep-Learning/https://github.com/KimythAnly/deeppacket PaperWithCode https://paperswithcode.com/paper/deep-packet-a-novel-approach-for-encryptedhttps://paperswithcode.com/paper/deep-learning-for-network-traffic GHM loss https://github.com/shuxinyin/NLP-Loss-Pytorch/blob/master/unbalanced_loss/GHM_loss.py LSTNet https://github.com/laiguokun/LSTNet Blogs 名称 网址 知乎流量分类总结 https://www.zhihu.com/people/yi-tun-4/posts 网络加密流量实验–基于原始流量 https://mathpretty.com/11408.html 利用神经网络进行网络流量识别 https://www.cnblogs.com/bonelee/p/10303108.html Encrypted traffic 加密流量分类任务进展综述 https://blog.csdn.net/weixin_41763134/article/details/104219211 流量分类方法设计（一）——参考论文整理 https://blog.csdn.net/pnnngchg/article/details/79826949 NLP大赛冠军总结：300万知乎多标签文本分类任务(附深度学习源码) http://www.360doc.com/content/17/1124/18/40903010_706805642.shtml 文本分类概述（nlp） https://blog.csdn.net/u014248127/article/details/80774668 将分类网络流量特征转换为数值-ISCX VPN2016数据集 https://www.pythonheidong.com/blog/article/786185/bb4b0cfd6260ffd780da/ 关于深度学习的网络流量分类论文整理（二） https://blog.csdn.net/caiguanhong/article/details/110292236?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.pc_relevant_antiscanv2&spm=1001.2101.3001.4242.1&utm_relevant_index=3 读文献—机器学习应用到网络流量分类综述 https://blog.csdn.net/weixin_43129841/article/details/121195735 5分钟理解Focal Loss与GHM——解决样本不平衡利器 https://zhuanlan.zhihu.com/p/80594704 PyTorch练习（二） 长短时记忆网络(LSTM) https://zhuanlan.zhihu.com/p/259650072 PyTorch的nn.LSTM使用说明 https://blog.csdn.net/github_31101389/article/details/107093867 LSTNET——deep time series models的集大成者 https://zhuanlan.zhihu.com/p/467944750 Papers 名称 网址 【2022年通信业经济运行情况之五 移动互联网流量快速增长】-国家发展和改革委员会 https://www.ndrc.gov.cn/fzggw/jgsj/gjss/sjdt/202201/t20220106_1311526.html?code=&state=123 2021年1~10月通信业经济运行情况-【维普期刊官网】 http://qikan.cqvip.com/Qikan/Article/Detail?id=7106372868 基于深度学习的加密流量分类研究综述 - 中国知网 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=JYXH202108020&uniplatform=NZKPT&v=y1kdKYHGttl6RkCkLqM_NjfJtCVzGpWODHSDCv9D9FHGSUFJwmwqDICJsZd9fuy6 PERT: Payload Encoding Representation from Transformer for Encrypted Traffic Classification https://www.zte.com.cn/global/about/magazine/zte-communications/2021/en202104/ResearchPaper/en202104010.html 基于深度学习的网络流量分类研究 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFDTEMP&filename=1021855723.nh&uniplatform=NZKPT&v=lwfkkv6d_nbjJlZYLvJw5tP_hFtb78227vxGXK0n9ZrwRazEzVZB9iMF3DS5d9gY 自注意力的多特征网络流量异常检测与分类 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=HDSZ202106016&uniplatform=NZKPT&v=1Q8AQAHIjB5I8oC008gC2wG1t1mWGdGgzg9-SoCkWcCKgGscaT-QNwQOpqalq1Pn 基于多层双向SRU与注意力模型的加密流量分类方法 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=JSJC2022022300H&uniplatform=NZKPT&v=SoV27XeYZugzFYILQLTmSO6NQ7tOhAKLtBGby3YnA-_ILYTRi_fRfPl3bBF1_hd0 基于深度学习的网络流量分类系统的研究与实现 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202201&filename=1021124063.nh&uniplatform=NZKPT&v=frLKwSePYe7UTBLgp3W8R0ejb6prUQlENBCZIUoSshwMuQRkK12hXBAubQicvtl6 基于卷积注意力门控循环网络的加密流量分类方法 https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2021&filename=XXCN202107007&uniplatform=NZKPT&v=FpR6QUdRBpzh4clnsUrl5XrgmPKh6IWyeBxvrzHadyvY9U4auRUWpfB-Dn7rWkOE VPN Network Traffic Classification Using Entropy Estimation and Time-Related Features https://link.springer.com/chapter/10.1007/978-981-16-3945-6_50 Others http://mawi.wide.ad.jp/mawi/ DanteSU            updated 2022-06-05 10:34:11 "},"DP/StyleTransfer/":{"url":"DP/StyleTransfer/","title":"姿态迁移","keywords":"","body":"姿态迁移 Basic Information Lead-in DanteSU            updated 2022-06-05 20:59:30 "},"DP/StyleTransfer/1.html":{"url":"DP/StyleTransfer/1.html","title":"Info","keywords":"","body":"Info DanteSU            updated 2022-06-05 10:34:47 "},"DP/CRS/":{"url":"DP/CRS/","title":"对话推荐","keywords":"","body":"对话式推荐系统 Basic Information Lead-in DanteSU            updated 2022-06-05 20:59:03 "},"DP/CRS/1.html":{"url":"DP/CRS/1.html","title":"Info","keywords":"","body":"Info Related Papers 名称 网址 Review-augmented Conversational Recommendation https://arxiv.org/pdf/2106.00957.pdf Towards Emotional Support Dialog Systems https://arxiv.org/abs/2106.01144 Why You Should Listen to This Song: Reason Generation for Explainable Recommendation https://ieeexplore.ieee.org/document/8637420 INSPIRED:Towards Sociable Recommendation Dialog System https://arxiv.org/abs/2009.14306 CRSLab: An Open-Source Toolkit for Building Conversational Recommender System https://arxiv.org/abs/2101.00939 DEUX: An Attribute-Guided Framework for Sociable Recommendation Dialog Systems https://arxiv.org/abs/2105.00825 A Model of Social Explanations for a Conversational Movie Recommendation System http://eprints.gla.ac.uk/193937/ Related Links 名称 网址 CRSLab：可能是最适合你的对话推荐系统开源库 https://blog.csdn.net/WYDQXCG/article/details/112361106 转载 对话推荐系统综述论文，A Survey on CRS https://zhuanlan.zhihu.com/p/127030405 Towards Explainable Conversational Recommendation https://www.aminer.cn/pub/5ef96b048806af6ef277218c/towards-explainable-conversational-recommendationhttps://www.bilibili.com/video/av969740822/ 对话推荐系统相关论文分类整理 https://zhuanlan.zhihu.com/p/383494625 A Survey on Conversational Recommender Systems https://blog.csdn.net/qq_39609267/article/details/108707734https://blog.csdn.net/weixin_43993244/article/details/106845080 对话推荐系统_RSPapers + 新增对话推荐系统论文合集 https://blog.csdn.net/weixin_39643865/article/details/111331361 推荐算法最前沿+CIKM2020推荐系统论文一览 https://blog.csdn.net/abcdefg90876/article/details/109040152 京东 对话推荐系统调研 https://blog.csdn.net/weixin_45519842/article/details/119770014 CRS Tutorial at IJCAI2021 https://web-ainf.aau.at/pub/jannach/slides/Tutorial-Conversational-Recommender-Systems-IJCAI-2021.pdf DanteSU            updated 2022-06-05 00:53:40 "},"DP/CG/":{"url":"DP/CG/","title":"计算机图形学","keywords":"","body":"计算机图形学 Basic Information Notes Lead-in 相机成像几何，投影几何 计算机图形学 Math for Computer Graphics Greg Turk, August 2019 Twenty-two years ago, I wrote an essay about what math is important for computer graphics. That document is now fairly dated, and I have decided that it is time to re-visit this question. I am writing this essay in part for college students who want to know what courses may be relevant to the study of computer graphics. For this reason, I will remark on the departments that are likely to offer courses in a given topic. Hopefully it is obvious that you do not need to be a college student to read this essay! Computer graphics draws upon many different areas of mathematics for tools that help accomplish various computational tasks. For as long as you want to pursue computer graphics, you should also plan to continue to learn more mathematical techniques. There are very few corners of computer graphics that do not make use of some form of mathematics. The most important point that I want to convey in this essay is the following. The mathematical topics that are often the most useful to graphics are so-called Numerical Methods. These are the tools that take abstract mathematical concepts (differentiation, integration, matrix inversion, etc.) and turn them into concrete algorithms that we can use to find numerical results to the problem at hand. When you first learn in calculus class how to differentiate and integrate, you start by doing this symbolically. (For example, the derivative of the sine function is cosine.) In graphics, we need to be able to translate the symbolic answer to a given problem into a numerical technique that can be implemented on the computer. For this reason, it is most often the applied mathematics courses (not those in pure mathematics)that are the most relevant to graphics. The numerical methods that are useful for graphics are frequently the same tools that various engineers use. This means that sometimes the most useful courses for graphics may not be in the math department. They may instead be found in other departments such as electrical engineering or mechanical engineering. In this essay I am going to refer the four core areas of computer graphics. These areas are: Modeling - creating 3D shape descriptions of objects Animation - making objects move Image Synthesis, also called Rendering - making pictures from 3D shapes Image and Video Manipulation I am going to visit the mathematics useful to graphics in an order that (approximately) lines up with the order of the four topics listed above. Note that modeling and animation often make use of similar mathematics. The same is true of the other pair — image synthesis and image/video manipulation often use similar math tools. Before I visit any of these topics, however, I am going to start with the math needed for a first course in graphics. Mathematical Basics: Linear Algebra and Trigonometry The most important topics for starting out in graphics are Linear Algebra and Trigonometry. We usually describe the location of a 3D graphics object according to its x, y and z coordinates. We can then apply the following operations on a 3D object: translate (move), scale (change size), and rotate. Translation and scale are accomplished using addition and multiplication, respectively. Rotation is done using sine and cosine, hence the need for trigonometry. The x, y and z coordinates of an object can be conveniently represented as a 3D vector, and the translate, scale and rotate operations can be described as multiplication by a matrix (of size 3x3 or 4x4). This is one of the reasons that a background in linear algebra is important for starting in graphics. Several other concepts from linear algebra also are useful, including matrix inversion, dot product, and cross product. Multivariable Calculus Many of the more advanced topics in computer graphics make use of the tools of Multivariable Calculus. These topics are usually saved for a second or third course in calculus. Many of the representations that are used in computer graphics are functions of multiple variables, and thus require tools to reason about derivatives and integrals of such functions. If you want to study computer graphics beyond a first course in the area, I strongly recommend taking the full sequence of calculus classes that your school offers. Differential Geometry Differential Geometry is the measurement of properties of curves and surfaces, and these techniques are very important for modeling in graphics. Common graphics-related tasks that fall under this domain include determining tangents, measuring curvature, evaluating lengths and areas, and finding shortest paths. Often differential geometry techniques are combined with optimization methods (more on this below). Fortunately, many math departments offer an undergraduate course in differential geometry. Computational Geometry Computational Geometry is the study of algorithms that efficiently and robustly solve geometric problems. Some common problems in this area include find convex hulls, finding nearest neighbors to a given query point, determining the intersection between two surfaces,and triangulating a polygon. The tools of computational geometry are frequently used in both modeling and animation (e.g for collision detection). Strictly speaking, computational geometry is a branch of computer science theory, not mathematics. You are more likely to find a course in computational geometry in a computer science department rather than in a math department. Numerical Linear Algebra The one topic in applied mathematics that is perhaps the most important across a wide array of graphics problems is Numerical Linear Algebra. Usually the study of numerical methods for linear algebra is typically not covered in a first course in linear algebra. The linear algebra problems that arise from computer graphics often require setting up and solving large linear systems of equations, with very large matrices and thousands or tens of thousands of unknowns. The simple methods that you learn for solving matrix equations in a first linear algebra course do not work for such problems. Instead, you need to learn to describe the linear systems in a sparse matrix form (much more memory efficient) and learn about iterative techniques for solving such systems. Some of these methods used include Jacobi, Gauss-Seidel, and the conjugate gradient method. Occasionally you may run into other related numerical problems such as finding eigenvectors and eigenvalues. Optimization Many problems in both modeling and animation describe a given task as an Optimization problem. Say we wish to create a smooth object that passes through a given set of points. First, the object in question is represented numerically, such as a collection of triangles that describes the shape of the surface. Next, we represent a desired quality of the object numerically, such as the smoothness of the surface. The problem is now to find the positions of the triangles’ vertices that maximizes the smoothness measure, while still passing through the given set of points. Such a minimization problem is described as a large linear system of equations, and iterative numerical techniques are used to solve such a system. Partial Differential Equations Animation of materials such as water, rubber and snow require numerical methods for Partial Differential Equations (PDE’s). The equations that arise from these problems include diffusion equations, transport equations, Laplace equations and Poisson equations. These are often solved by turning the problem into a large linear system of equations, or formulating the problem as one of constrained optimization. You are unlikely to learn much about these techniques in a calculus class. The techniques for solving such problems are more often studied in engineering courses and numerical methods courses. A well-known method for solving some of these problems is know as the Finite Element Method (FEM). Although it is by no means the only method for solving some of these problems, it is one of the more important techniques, and there are often courses devoted to this approach. Not only are these numerical techniques important for computer animation, they also arise frequently in 3D modeling problems. Ordinary Differential Equations The animation of characters (people, animals, robots) is often performed by representing the character as a collection of rigid objects that are connected by joints. For example, a person’s arm can be described as an upper arm segment, a lower arm segment, and an elbow joint that connects these two segments. The motion of a character described in this way is governed by the numerical integration of Ordinary Differential Equations (ODE’s). Alas, a typical course in ODE’s will most likely give you very little in the way of help for this, because such courses are heavy on symbolic solutions instead of numerical solutions. A course on numerical methods is much more likely to discuss the relevant numerical methods (forward Euler, midpoint method, implicit integration, Runge-Kutta, etc.) Signal Processing Many areas in image synthesis and image manipulation touch upon signal processing. Indeed, these techniques are sometimes also relevant to modeling and animation as well. We usually represent an image as 2D grid of pixels, where each pixel is given a color. This regular array of color values can be thought of as a digital representation of a 2D function, and this is a “signal”. We can perform operations on our image (the signal) such as contrast modification, blurring, warping, sharpening, and so on. The shape of a surface or the motion of an animated character can also be thought of as a signal, making these techniques relevant to modeling and animation as well. Often the best way to analyze and process signals is to convert them into another representation by using tools such as the Fourier transform. Signal processing is heavily used in the study of electronics and audio, so courses on this topic are often taught in an electrical engineering department. Monte Carlo Integration Methods While the problems in animation usually lead to differential equations, those of image synthesis are usually integral equations. The amount of light that reaches a light-detecting element in a camera or our eye is the sum of all the light coming from all different directions, and that light may have come from several different light sources and bounced off a number of different materials. Such as sum of light paths can be written as an integral equation. While you may learn of basic quadrature methods for calculating integrals in an introductory calculus class, it turns out that such methods do not work well for light transport problems. Instead, random sampling of many different light paths is a much better way to go. These techniques are referred to as Monte Carlo Methods, and these randomization techniques were named for the resort of the same name where gambling casinos are big business. Unfortunately, courses on Monte Carlo techniques are pretty rare. The Rise of Machine Learning If you are studying computer science, you are undoubtably aware that the area of machine learning has recently become huge. (I am writing this in 2019.) In particular, the methods of deep neural networks have seen an explosion of activity. It probably comes as no surprise that deep learning has had a large impact in computer graphics. Neural networks are used for numerous graphics tasks, including: where to shoot rays for better lighting calculations, denoising images, controlling the motion of virtual characters, classifying 3D models, and for image editing. If you want to study graphics, it is important to learn the tools of machine learning, and especially to learn about neural networks. Note that machine learning is closely related the mathematical topics of probability and statistics. Off The Beaten Path Some topics in mathematics are not as commonly used in graphics as those that I have mentioned above. In the old version of this essay, I said that Topology and Abstract Algebrawere not useful for graphics. Now I have to correct myself. Topology As it turns out, one of my own PhD students, Eugene Zhang, did his dissertation work in computer graphics that drew primarily from the area of topology. He studied how to create and edit vector and tensor fields based on the critical points of the fields. Analyzing the connections between these critical points is very much a problem of topology. His work is not the only such case, and there are several other techniques in graphics that draw heavily upon ideas from topology. Abstract Algebra Abstract algebra is the study of objects such as groups, rings and fields. While many of these mathematical constructs are not particularly useful to computer graphics, a researcher named Ken Turkowski pointed out to me that group theory does in fact play an important role in graphics. When we describe the orientation of a 3D object, and when we want to change its orientation, we are using group theory. The space of all 3D orientations is known as the group SO(3), and it turns out this is a fairly counter-intuitive mathematical object. Researchers in graphics have used several different ways of describing elements in this group and operations over these elements, including 3x3 matrices, quaternions, and exponential maps. Describing smooth changes in orientation often leads graphics researchers to study SO(3). Number Theory The topic of number theory is the study of the integers, and researchers in this area investigates questions such as the distribution of prime numbers. Famously, Fermat’s Last Theorem (now solved!) is a problem in number theory. The mathematician G. H. Hardy wrote a book entitled “A Mathematician’s Apology”, in which he describes the beauty of pure mathematics. One of the themes of his book is that his own area of expertise, number theory, is a topic that is to be appreciated in and of itself. He goes on to say that number theory really doesn’t have much in the way of practical applications to real-world problems. So far as I know, number theory is not particularly useful in computer graphics. If you choose to study number theory, you should do it for the beauty of the topic and not for any possible application in graphics. DanteSU            updated 2022-06-05 10:21:25 "},"DP/CG/1.html":{"url":"DP/CG/1.html","title":"Info","keywords":"","body":"Info Related Links 名称 网址 SIGGRAPH 2021 https://www.neuralrender.com Instant Neural Graphics Primitives https://github.com/NVlabs/instant-ngp 物理仿真中的符号距离场（SDF） https://zhuanlan.zhihu.com/p/390625164 【译】Signed Distance Fields(有符号的距离场) https://zhuanlan.zhihu.com/p/357606643 视网膜Retina技术 https://www.cnblogs.com/constantince/p/15475408.html DanteSU            updated 2022-06-05 20:40:19 "},"DP/CG/2.html":{"url":"DP/CG/2.html","title":"Notes","keywords":"","body":"Notes DanteSU            updated 2022-06-05 15:22:59 "}}